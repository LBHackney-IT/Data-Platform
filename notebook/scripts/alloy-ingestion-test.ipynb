{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6015ef2a-cd82-481e-8508-8fd36bbf831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import zipfile\n",
    "import html\n",
    "import boto3\n",
    "import datetime\n",
    "from pyspark.sql import functions as f\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.sql import SQLContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.job import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb171c-3567-4f2e-b205-8160c54c3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_to_df(file_id, api_key, filename):\n",
    "    url_download = f'https://api.uk.alloyapp.io/api/file/{file_id}?token={api_key}'\n",
    "    r = requests.get(url_download, headers=headers)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    df = pd.read_csv(html.unescape(z.extract(member=filename)), index_col=False)\n",
    "    return df\n",
    "\n",
    "def get_last_import_date_time(glue_context, database, resource):\n",
    "    if not table_exists_in_catalog(glue_context, resource, database):\n",
    "        print(f\"Couldn't find table {resource} in database {database}.\")\n",
    "        return datetime.datetime(1970, 1, 1)\n",
    "    print(f\"found table for {resource} in {database}\")\n",
    "    return glue_context.sql(f\"SELECT max(import_datetime) as max_import_date_time FROM `{database}`.{resource}\").take(1)[0].max_import_date_time\n",
    "\n",
    "def format_time(date_time):\n",
    "    t = date_time.strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
    "    return t[:-3]+\"Z\"\n",
    "  \n",
    "def update_aqs(alloy_query, last_import_date_time):\n",
    "    child_value = [{\"type\": \"GreaterThan\", \"children\": [{\"type\": \"ItemProperty\", \"properties\": {\"itemPropertyName\": \"lastEditDate\"}}, {\"type\": \"DateTime\", \"properties\": {\"value\": []}}]}]\n",
    "    child_value[0]['children'][1]['properties']['value'] = [last_import_date_time]\n",
    "    alloy_query['aqs']['children'] = child_value\n",
    "    return alloy_query \n",
    "\n",
    "def get_task_id(response):\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Request unsuccessful while getting task id with status code: {response.status_code}\")\n",
    "        return \n",
    "    \n",
    "    json_output = json.loads(response.text)\n",
    "    task_id = json_output[\"backgroundTaskId\"]\n",
    "    return task_id\n",
    "\n",
    "def get_task_status(response):\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Request unsuccessful while getting task status with status code: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    json_output = json.loads(response.text)\n",
    "    task_status = json_output[\"task\"][\"status\"]\n",
    "    return task_status\n",
    "\n",
    "def get_file_item_id(response):\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Request unsuccessful while getting file item id with status code: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    json_output = json.loads(response.text)\n",
    "    file_id = json_output[\"fileItemId\"]\n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8149ac-c224-4eca-8019-f82770943d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def table_exists_in_catalog(glue_context, table, database):\n",
    "    tables = glue_context.tables(database)\n",
    "\n",
    "    return tables.filter(tables.tableName == table).count() == 1\n",
    "\n",
    "def normalize_column_name(column: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize column name by replacing all non alphanumeric characters with underscores\n",
    "    strips accents and make lowercase\n",
    "    :param column: column name\n",
    "    :return: normalized column name\n",
    "    Example of applying: df.columns = map(clean_column_names, panada_df.columns)\n",
    "    \"\"\"\n",
    "    formatted_name = format_name(column)\n",
    "    return unicodedata.normalize('NFKD', formatted_name).encode('ASCII', 'ignore').decode()\n",
    "\n",
    "def format_name(col_name):\n",
    "    non_alpha_num_chars_stripped = re.sub('[^a-zA-Z0-9]+', \"_\", col_name)\n",
    "    no_trailing_underscores = re.sub(\"_$\", \"\", non_alpha_num_chars_stripped)\n",
    "    return no_trailing_underscores.lower()\n",
    "\n",
    "def convert_pandas_df_to_spark_dynamic_df(sql_context, panadas_df):\n",
    "    # Convert to SparkDynamicDataFrame\n",
    "    spark_df = sql_context.createDataFrame(panadas_df)\n",
    "    spark_df = spark_df.coalesce(1)\n",
    "    return spark_df\n",
    "\n",
    "def add_import_time_columns(data_frame):\n",
    "    now = datetime.datetime.now()\n",
    "    importYear = str(now.year)\n",
    "    importMonth = str(now.month).zfill(2)\n",
    "    importDay = str(now.day).zfill(2)\n",
    "    importDate = importYear + importMonth + importDay\n",
    "\n",
    "    data_frame = data_frame.withColumn(\n",
    "        'import_datetime', f.current_timestamp())\n",
    "    data_frame = data_frame.withColumn(\n",
    "        'import_timestamp', f.lit(str(now.timestamp())))\n",
    "    data_frame = data_frame.withColumn('import_year', f.lit(importYear))\n",
    "    data_frame = data_frame.withColumn('import_month', f.lit(importMonth))\n",
    "    data_frame = data_frame.withColumn('import_day', f.lit(importDay))\n",
    "    data_frame = data_frame.withColumn('import_date', f.lit(importDate))\n",
    "    return data_frame\n",
    "\n",
    "PARTITION_KEYS = ['import_year', 'import_month', 'import_day', 'import_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ff554-33fb-45ab-aff8-4a88ef5f258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext.getOrCreate()\n",
    "glue_context = GlueContext(sc)\n",
    "#logger = glue_context.get_logger()\n",
    "job = Job(glue_context)\n",
    "#job.init(args['JOB_NAME'], args)\n",
    "sparkContext = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sparkContext)\n",
    "sqlContext = SQLContext(sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee6e9c-ad42-4b14-be60-6865839bd0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = \"dw_education_and_compliance_inspection\"\n",
    "bucket_target = \"dataplatform-stg-raw-zone\"\n",
    "api_key = \"f1e18002-5743-4aca-9d59-90c8276866d3\"\n",
    "database = \"dataplatform-stg-env-services-raw-zone\"\n",
    "prefix = \"env-services/alloy/api-responses/\"\n",
    "aqs = {\"aqs\":{\n",
    "  \"type\": \"Join\",\n",
    "  \"properties\": {\n",
    "    \"attributes\": [\n",
    "      \"attributes_itemsTitle\",\n",
    "      \"attributes_itemsSubtitle\",\n",
    "      \"attributes_itemsGeometry\",\n",
    "      \"attributes_inspectionsInspectionNumber\",\n",
    "      \"attributes_tasksStatus\",\n",
    "      \"attributes_tasksTeam\",\n",
    "      \"attributes_tasksRaisedTime\",\n",
    "      \"attributes_tasksStartTime\",\n",
    "      \"attributes_tasksCompletionTime\",\n",
    "      \"attributes_wasteEducationInspectionServiceOutcome_6032eba956a338006661f6f8\",\n",
    "      \"attributes_wasteEducationInspectionResidentAvailable_6034de1cca290e006b10eaa5\",\n",
    "      \"attributes_wasteEducationInspectionBarriersToRRW_6034e4f16668f2006c62013b\",\n",
    "      \"attributes_wasteEducationInspectionEnforcementOutcome_6036a88b267b37006a951ac8\",\n",
    "      \"attributes_wasteEducationInspectionEnforcementIssue_603c11fa306e42000a19ee8e\",\n",
    "      \"attributes_tasksDescription\"\n",
    "    ],\n",
    "    \"collectionCode\": [\n",
    "      \"Live\"\n",
    "    ],\n",
    "    \"dodiCode\": \"designs_wasteEducationInspection_6032eb1356a338006661f6e4\",\n",
    "    \"joinAttributes\": [\n",
    "      \"root.attributes_tasksStatus.attributes_taskStatusesStatus\",\n",
    "      \"root.attributes_tasksTeam.attributes_teamsTeamName\",\n",
    "      \"root.attributes_wasteEducationInspectionServiceOutcome_6032eba956a338006661f6f8.attributes_serviceOutcomeServiceOutcome_602eaaad3cf282006c40f4f0\",\n",
    "      \"root.attributes_wasteEducationInspectionBarriersToRRW_6034e4f16668f2006c62013b.attributes_barriersToRRWBarriersToRRW_6034e1c96668f2006c61f949\",\n",
    "      \"root.attributes_wasteEducationInspectionEnforcementOutcome_6036a88b267b37006a951ac8.attributes_enforcementOutcomeEnforcementOutcome_602ea67d3cf282006c40f3f3\",\n",
    "      \"root.attributes_wasteEducationInspectionEnforcementIssue_603c11fa306e42000a19ee8e.attributes_enforcementIssueEnforcementIssue_603c0f135b27c7000aed8475\"\n",
    "    ],\n",
    "    \"sortInfo\": {\n",
    "      \"attributeCode\": \"attributes_inspectionsInspectionNumber\",\n",
    "      \"sortOrder\": \"Ascending\"\n",
    "    }\n",
    "  }\n",
    "},\"fileName\":\"DW Education&Compliance Inspection.csv\",\"exportHeaderType\":\"Name\"}\n",
    "filename = \"DW Education&Compliance Inspection/DW Education&Compliance Inspection.csv\"\n",
    "last_import_date_time = format_time(datetime.datetime(2020, 5, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad7d2c-7ae5-4e89-b09e-f132acb6ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_target_url = \"s3://\" + bucket_target + \"/\" +  prefix + resource + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb188a-e30b-406a-adcb-2202ef64f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Accept': 'application/json', 'Content-Type': 'application/json'}\n",
    "region = 'uk'\n",
    "post_url = f'https://api.{region}.alloyapp.io/api/export/?token={api_key}'\n",
    "aqs = update_aqs(aqs, last_import_date_time)\n",
    "response = requests.post(post_url, data=json.dumps(aqs), headers=headers)\n",
    "  \n",
    "task_id = get_task_id(response)\n",
    "url = f'https://api.{region}.alloyapp.io/api/task/{task_id}?token={api_key}'\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e6252-edf6-49b5-b6e9-1ff3af86f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(aqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1582aa56-b81f-4263-9b15-d4b796bb3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_status = ''\n",
    "file_id = ''\n",
    "while task_status != 'Complete':\n",
    "    time.sleep(60)\n",
    "    response = requests.get(url)\n",
    "    task_status = get_task_status(response)\n",
    "    print(f\"task status: {task_status}\")\n",
    "\n",
    "else:\n",
    "    url = f'https://api.{region}.alloyapp.io/api/export/{task_id}/file?token={api_key}'\n",
    "    response = requests.get(url) \n",
    "    file_id = get_file_item_id(response)\n",
    "    \n",
    "    print(\"downloading file to df\")\n",
    "    pandasDataFrame = download_file_to_df(file_id, api_key, filename)\n",
    "    \n",
    "    print(\"processing DF\")\n",
    "    all_columns = list(pandasDataFrame)\n",
    "    pandasDataFrame[all_columns] = pandasDataFrame[all_columns].astype(str)\n",
    "    # Replace missing column names with valid names\n",
    "    pandasDataFrame.columns = [\"column\" + str(i) if a.strip() == \"\" else a.strip() for i, a in enumerate(pandasDataFrame.columns)]\n",
    "    pandasDataFrame.columns = map(normalize_column_name, pandasDataFrame.columns)\n",
    "    sparkDynamicDataFrame = convert_pandas_df_to_spark_dynamic_df(sqlContext, pandasDataFrame)\n",
    "    sparkDynamicDataFrame = sparkDynamicDataFrame.replace('nan', None).replace('NaT', None)\n",
    "    sparkDynamicDataFrame = sparkDynamicDataFrame.na.drop('all') # Drop all rows where all values are null NOTE: must be done before add_import_time_columns\n",
    "    sparkDynamicDataFrame = add_import_time_columns(sparkDynamicDataFrame)\n",
    "    dataframe = DynamicFrame.fromDF(sparkDynamicDataFrame, glueContext, \"alloyDWeducation\")\n",
    "    parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "        frame=dataframe,\n",
    "        connection_type=\"s3\",\n",
    "        connection_options={\"path\": s3_target_url, \"partitionKeys\": PARTITION_KEYS},\n",
    "        format=\"parquet\",\n",
    "        transformation_ctx=f\"alloy_{resource}_sink\"\n",
    "    )\n",
    "    print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a67ea-49ce-4c38-afd6-5f916cb895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"processing DF\")\n",
    "all_columns = list(pandasDataFrame)\n",
    "pandasDataFrame[all_columns] = pandasDataFrame[all_columns].astype(str)\n",
    "# Replace missing column names with valid names\n",
    "pandasDataFrame.columns = [\"column\" + str(i) if a.strip() == \"\" else a.strip() for i, a in enumerate(pandasDataFrame.columns)]\n",
    "pandasDataFrame.columns = map(normalize_column_name, pandasDataFrame.columns)\n",
    "sparkDynamicDataFrame = convert_pandas_df_to_spark_dynamic_df(sqlContext, pandasDataFrame)\n",
    "sparkDynamicDataFrame = sparkDynamicDataFrame.replace('nan', None).replace('NaT', None)\n",
    "sparkDynamicDataFrame = sparkDynamicDataFrame.na.drop('all') # Drop all rows where all values are null NOTE: must be done before add_import_time_columns\n",
    "sparkDynamicDataFrame = add_import_time_columns(sparkDynamicDataFrame)\n",
    "dataframe = DynamicFrame.fromDF(sparkDynamicDataFrame, glueContext, \"alloyDWeducation\")\n",
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=dataframe,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"path\": s3_target_url, \"partitionKeys\": PARTITION_KEYS},\n",
    "    format=\"parquet\",\n",
    "    transformation_ctx=f\"alloy_{resource}_sink\"\n",
    ")\n",
    "print(\"all done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue Spark - Local (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
