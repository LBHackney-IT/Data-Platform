{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    }
   ],
   "source": [
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('James1')\n",
    "\n",
    "sys.argv.append('--perfect_match_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-landing-zone/data-and-insight/address-matching-glue-job-output/perfect_match_s3_bucket_target')\n",
    "\n",
    "sys.argv.append('--best_match_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-landing-zone/data-and-insight/address-matching-glue-job-output/best_match_s3_bucket_target')\n",
    "\n",
    "sys.argv.append('--non_match_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-landing-zone/data-and-insight/address-matching-glue-job-output/non_match_s3_bucket_target')\n",
    "\n",
    "sys.argv.append('--imperfect_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-landing-zone/data-and-insight/address-matching-glue-job-output/imperfect_s3_bucket_target')\n",
    "\n",
    "sys.argv.append('--query_addresses_url')\n",
    "sys.argv.append('s3://dataplatform-stg-landing-zone/data-and-insight/address-matching-test/test_addresses.gz.parquet')\n",
    "\n",
    "sys.argv.append('--target_addresses_url')\n",
    "sys.argv.append('s3://dataplatform-stg-landing-zone/data-and-insight/address-matching-test/addresses_api_full.gz.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, HashingTF, MinHashLSH\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "import pyspark.sql.functions as F\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "## write into the log file with:\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "# Environment Arguments\n",
    "perfect_match_s3_bucket_target = get_glue_env_var('perfect_match_s3_bucket_target', '')\n",
    "best_match_s3_bucket_target = get_glue_env_var('best_match_s3_bucket_target', '')\n",
    "imperfect_s3_bucket_target = get_glue_env_var('imperfect_s3_bucket_target', '')\n",
    "non_match_s3_bucket_target = get_glue_env_var('non_match_s3_bucket_target', '')\n",
    "\n",
    "query_addresses_url = get_glue_env_var('query_addresses_url', '')\n",
    "target_addresses_url = get_glue_env_var('target_addresses_url', '')\n",
    "\n",
    "# Context Setup\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "\n",
    "job.init('James1', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger.info('fetch first set of data')\n",
    "query_addresses = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\n",
    "        \"paths\": [query_addresses_url],\n",
    "        \"recurse\": True\n",
    "    },\n",
    "    transformation_ctx=\"query_addresses\"\n",
    ")\n",
    "query_addresses_df = query_addresses.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('concat first set of data')\n",
    "query_concat = query_addresses_df.withColumn(\n",
    "    \"concat_address\",\n",
    "    F.concat_ws(\" \", \"concatenated_string_to_match\", \"postcode\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('fetch second set of data')\n",
    "target_addresses = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"paths\": [target_addresses_url], \"recurse\": True},\n",
    "    transformation_ctx=\"target_addresses\"\n",
    ")\n",
    "target_addresses_df = target_addresses.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('concat seconds set of data')\n",
    "target_concat_address = target_addresses_df.withColumn(\n",
    "    \"concat_address\",\n",
    "    F.concat_ws(\" \", \"concatenated2\", \"postcode\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would be nice to make this optional: that creates a subset of LLPG based on the postcodes in the list of addresses to match\n",
    "target_concat_address = target_concat_address.join(query_concat, \"postcode\",\"leftsemi\")\n",
    "\n",
    "model = Pipeline(stages=[\n",
    "    RegexTokenizer(\n",
    "        pattern=\"\", inputCol=\"concat_address\", outputCol=\"tokens\", minTokenLength=1\n",
    "    ),\n",
    "    NGram(n=3, inputCol=\"tokens\", outputCol=\"ngrams\"),\n",
    "    HashingTF(inputCol=\"ngrams\", outputCol=\"vectors\"),\n",
    "    MinHashLSH(inputCol=\"vectors\", outputCol=\"lsh\")\n",
    "]).fit(target_concat_address)\n",
    "\n",
    "target_hashed = model.transform(target_concat_address)\n",
    "query_hashed = model.transform(query_concat)\n",
    "\n",
    "joined = model.stages[-1].approxSimilarityJoin(\n",
    "    query_hashed.withColumnRenamed(\"concat_address\", \"query_address\"),\n",
    "    target_hashed.withColumnRenamed(\"concat_address\", \"result_address\"),\n",
    "    1 # when in the restricted mode (line 77), it's better to set the threshold to 0.7\n",
    ")\n",
    "\n",
    "window = Window.partitionBy(joined['datasetA.query_address']).orderBy(joined['distCol'])\n",
    "ranked = joined.select('*', rank().over(window).alias('rank'))\n",
    "ranked_frame = ranked.select(\n",
    "    'datasetA.query_address',\n",
    "    'datasetB.result_address',\n",
    "    'datasetB.uprn',\n",
    "    'distCol',\n",
    "    'rank',\n",
    "    'datasetA.prinx'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep best match and remove duplicates\n",
    "bestMatch = ranked_frame.filter(col('rank') == 1).dropDuplicates(['prinx'])\n",
    "nonMatch = query_concat.join(bestMatch, \"prinx\", \"leftanti\")\n",
    "perfectMatch = bestMatch.filter('distCol = 0')\n",
    "imperfectMatch = bestMatch.filter('distCol != 0')\n",
    "# results = bestMatch.union(nonMatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMatchframe = DynamicFrame.fromDF(bestMatch, glueContext, \"bestMatch\")\n",
    "nonMatchframe = DynamicFrame.fromDF(nonMatch, glueContext, \"nonMatch\")\n",
    "imperfectMatchframe = DynamicFrame.fromDF(imperfectMatch, glueContext, \"imperfectMatch\")\n",
    "perfectMatchframe = DynamicFrame.fromDF(perfectMatch, glueContext, \"perfectMatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=perfectMatch,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": perfect_match_s3_bucket_target, \"partitionKeys\": []},\n",
    "    transformation_ctx=\"parquetData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=bestMatchframe,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": best_match_s3_bucket_target, \"partitionKeys\": []},\n",
    "    transformation_ctx=\"parquetData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=imperfectMatch,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": imperfect_s3_bucket_target, \"partitionKeys\": []},\n",
    "    transformation_ctx=\"parquetData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=nonMatchframe,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": non_match_s3_bucket_target, \"partitionKeys\": []},\n",
    "    transformation_ctx=\"parquetData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
