{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.utils import getResolvedOptions\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, HashingTF, MinHashLSH\n",
    "from pyspark.sql.window import Window\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql.functions import col, max\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# from helpers import get_glue_env_var, PARTITION_KEYS\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "job = Job(glueContext)\n",
    "\n",
    "def get_latest_partitions(dfa):\n",
    "   dfa = dfa.where(col('import_year') == dfa.select(max('import_year')).first()[0])\n",
    "   dfa = dfa.where(col('import_month') == dfa.select(max('import_month')).first()[0])\n",
    "   dfa = dfa.where(col('import_day') == dfa.select(max('import_day')).first()[0])\n",
    "   return dfa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------------+-----------+--------+------+------+\n",
      "|emp_id|name|superior_emp_id|year_joined|postcode|gender|salary|\n",
      "+------+----+---------------+-----------+--------+------+------+\n",
      "+------+----+---------------+-----------+--------+------+------+"
     ]
    }
   ],
   "source": [
    "# pure testing\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\" \",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\" \",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\" \",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\" \",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\" \",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"postcode\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "\n",
    "\n",
    "\n",
    "empDF = empDF.withColumn(\"postcode\", F.regexp_replace(F.col(\"postcode\"), \"\\A +\\z\", ''))\n",
    "\n",
    "empDF.filter(\"postcode = ' '\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a classification function\n",
    "\n",
    "def match_type(levenshtein):\n",
    "    if levenshtein == 0:\n",
    "        return 'perfect match'\n",
    "    elif levenshtein > 15:\n",
    "        return 'unmatched'\n",
    "    else:\n",
    "        return 'imperfect match'\n",
    "\n",
    "# convert the classification function to a UDF Function by passing in the function and the return type of function (string in this case)\n",
    "# +++++++++++++++++++++++++\n",
    "udf_matchtype = F.udf(match_type, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "\"cannot resolve '`import_year`' given input columns: [];;\\n'Aggregate [max('import_year) AS max(import_year)#2]\\n+- LogicalRDD false\\n\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 20, in get_latest_partitions\n",
      "  File \"/home/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1320, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/home/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/home/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`import_year`' given input columns: [];;\\n'Aggregate [max('import_year) AS max(import_year)#2]\\n+- LogicalRDD false\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# addresses_api_data_database = get_glue_env_var('addresses_api_data_database', '')\n",
    "addresses_api_data_database = 'dataplatform-stg-raw-zone-unrestricted-address-api'\n",
    "# addresses_api_data_table = get_glue_env_var('addresses_api_data_table', '')\n",
    "addresses_api_data_table = 'unrestricted_address_api_dbo_hackney_address'\n",
    "# source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "source_catalog_database = 'housing-repairs-refined-zone'\n",
    "# source_catalog_table = get_glue_env_var('source_catalog_table', '')\n",
    "source_catalog_table = 'housing_repairs_elec_mech_fire_communal_lighting_address_cleaned'\n",
    "# source_catalog_table = 'housing_repairs_repairs_dlo_with_cleaned_addresses_with_cleaned_addresses'\n",
    "######################## NEW - possible values are allow/force/forbid\n",
    "match_to_property_shell = 'force' \n",
    "# source_catalog_table = 'housing-repairs-repairs-with-uprn-from-uhref_with_uprn_from_uhref'\n",
    "# target_destination = get_glue_env_var('target_destination', '')\n",
    "target_destination = 's3://dataplatform-stg-refined-zone/housing-repairs/repairs-dlo/with-matched-addresses/'\n",
    "\n",
    "query_addresses_ddf = glueContext.create_dynamic_frame.from_catalog(\n",
    "    name_space=source_catalog_database,\n",
    "    table_name=source_catalog_table,\n",
    ")\n",
    "\n",
    "query_addresses = query_addresses_ddf.toDF()\n",
    "query_addresses = get_latest_partitions(query_addresses)\n",
    "query_addresses.count()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_addresses.select('new_uhw_number').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_addresses_sample = query_addresses.where(\"concatenated_string_to_match != '' AND uprn IS NULL\")\n",
    "# +++++++++++++++++++++++++ SAMPLE\n",
    "# query_addresses_sample = query_addresses_sample.sample(0.01)\n",
    "query_addresses_sample = query_addresses.sample(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_addresses_sample.count()\n",
    "# query_addresses_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## NEW\n",
    "# look for no-number addresses which could be matched to parent shells \n",
    "# query_addresses_sample = query_addresses_sample.filter(col(\"concatenated_string_to_match\")\n",
    "#     .rlike(\"^\\D*$\")\n",
    "#   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_concat = query_addresses_sample.withColumn(\n",
    "    \"query_address\",\n",
    "    F.concat_ws(\" \", \"concatenated_string_to_match\", \"postcode\")\n",
    ")\n",
    "\n",
    "query_concat = query_concat.select(\"prinx\", \"query_address\", \"postcode\").withColumnRenamed(\"postcode\", \"query_postcode\")\n",
    "query_concat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREP ADDRESSES API DATA\n",
    "\n",
    "target_addresses_ddf = glueContext.create_dynamic_frame.from_catalog(\n",
    "    name_space=addresses_api_data_database,\n",
    "    table_name=addresses_api_data_table,\n",
    ")\n",
    "\n",
    "target_addresses = target_addresses_ddf.toDF()\n",
    "target_addresses = get_latest_partitions(target_addresses)\n",
    "\n",
    "######### NEW set property shell preferences\n",
    "if match_to_property_shell == 'force': \n",
    "    target_addresses = target_addresses.where(\"blpu_class LIKE 'P%'\")\n",
    "elif match_to_property_shell == 'forbid':\n",
    "    target_addresses = target_addresses.where(\"blpu_class NOT LIKE 'P%'\")\n",
    "#########    \n",
    "\n",
    "######### NEW add blpu class\n",
    "target_concat = target_addresses.select(\"line1\", \"line2\", \"line3\", \"postcode\", \"uprn\", \"blpu_class\")\n",
    "target_concat = target_concat.withColumn(\n",
    "    \"concat_lines\",\n",
    "    F.concat_ws(\" \", \"line1\", \"line2\", \"line3\")\n",
    ")\n",
    "\n",
    "target_concat = target_concat.withColumn(\"concat_lines\", F.trim(F.col(\"concat_lines\")))\n",
    "target_concat = target_concat.withColumn(\"address_length\", F.length(F.col(\"concat_lines\")))\n",
    "target_concat = target_concat.withColumn(\"concat_lines\",\n",
    "      F.when(F.col(\"concat_lines\").endswith(\" HACKNEY\"), F.expr(\"substring(concat_lines, 1, address_length -8)\"))\n",
    "          .otherwise(F.col(\"concat_lines\")))\n",
    "\n",
    "target_concat = target_concat.withColumn(\n",
    "    \"target_address\",\n",
    "    F.concat_ws(\" \", \"concat_lines\", \"postcode\")\n",
    ")\n",
    "\n",
    "target_concat = target_concat.withColumn(\n",
    "    \"target_address_short\",\n",
    "    F.concat_ws(\" \", \"line1\", \"postcode\")\n",
    ")\n",
    "\n",
    "target_concat = target_concat.withColumn(\n",
    "    \"target_address_medium\",\n",
    "    F.concat_ws(\" \", \"line1\", \"line2\", \"postcode\")\n",
    ")\n",
    "\n",
    "######### NEW version without postcode\n",
    "target_concat = target_concat.withColumn(\n",
    "    \"target_address_nopostcode\",\n",
    "    F.concat_ws(\" \", \"line1\", \"line2\")\n",
    ")\n",
    "\n",
    "######### NEW add blpu class\n",
    "target_concat = target_concat.select(\"target_address\", \"target_address_short\", \"target_address_medium\", \"target_address_nopostcode\", \"postcode\", \"uprn\", \"blpu_class\").withColumnRenamed(\"postcode\", \"target_postcode\")\n",
    "\n",
    "target_concat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_with_same_postcode = query_concat.join(target_concat, query_concat.query_postcode == target_concat.target_postcode, \"fullouter\")\n",
    "\n",
    "# query_concat_with_postcode = query_concat.filter(\"query_postcode != ' '\");\n",
    "query_concat_with_postcode = query_concat.withColumn(\"query_postcode\", F.regexp_replace(F.col(\"query_postcode\"), \"\\A +\\z\", ''))\n",
    "query_concat_with_postcode.query_concat.filter(\"query_postcode != ' '\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPARE QUERY AND TARGET TEST\n",
    "\n",
    "\n",
    "\n",
    "# target_concat_with_postcode = target_concat.filter(\"target_postcode IS NOT NULL\");\n",
    "cross_with_same_postcode = query_concat.join(target_concat, query_concat.query_postcode == target_concat.target_postcode, \"fullouter\")\n",
    "\n",
    "\n",
    "cross_compare = cross_with_same_postcode.withColumn(\"levenshtein\", F.levenshtein(F.col(\"query_address\"), F.col(\"target_address\")))\n",
    "cross_compare = cross_compare.withColumn(\"levenshtein_short\", F.levenshtein(F.col(\"query_address\"), F.col(\"target_address_short\")))\n",
    "cross_compare = cross_compare.withColumn(\"levenshtein_medium\", F.levenshtein(F.col(\"query_address\"), F.col(\"target_address_medium\")))\n",
    "cross_compare = cross_compare.withColumn(\"levenshtein_10char\", F.levenshtein(F.substring(F.col(\"query_address\"), 1, 10), F.substring(F.col(\"target_address\"), 1, 10)))\n",
    "\n",
    "cross_with_same_postcode.select(\"query_address\", \"query_postcode\", \"target_postcode\", \"target_address\").show()\n",
    "# query_concat_with_postcode.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_with_same_postcode.filter(\"target_postcode = query_postcode\").select(\"query_address\", \"target_address\", \"levenshtein\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_concat.filter(\"query_postcode != ' '\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPARE QUERY AND TARGET\n",
    "\n",
    "cross_with_same_postcode = query_concat.join(target_concat, query_concat.query_postcode == target_concat.target_postcode, \"fullouter\")\n",
    "\n",
    "\n",
    "cross_compare = cross_with_same_postcode.withColumn(\"levenshtein\", F.levenshtein(F.col(\"query_address\"), F.col(\"target_address\")))\n",
    "cross_compare = cross_compare.withColumn(\"levenshtein_short\", F.levenshtein(F.col(\"query_address\"), F.col(\"target_address_short\")))\n",
    "cross_compare = cross_compare.withColumn(\"levenshtein_medium\", F.levenshtein(F.col(\"query_address\"), F.col(\"target_address_medium\")))\n",
    "cross_compare = cross_compare.withColumn(\"levenshtein_10char\", F.levenshtein(F.substring(F.col(\"query_address\"), 1, 10), F.substring(F.col(\"target_address\"), 1, 10)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ROUND 0 - look for perfect\n",
    "perfectFull = cross_compare.filter(\"levenshtein = 0\").dropDuplicates(['prinx'])\n",
    "perfectFull = perfectFull.withColumn(\"match_type\", udf_matchtype(\"levenshtein\"))\n",
    "perfectFull = perfectFull.select(F.col(\"prinx\"), F.col(\"query_address\"), F.col(\"uprn\").alias(\"matched_uprn\"), F.col(\"target_address\").alias(\"matched_address\"), \"match_type\")\n",
    "\n",
    "\n",
    "perfectShort = cross_compare\\\n",
    "    .join(perfectFull, \"prinx\", \"left_anti\")\\\n",
    "    .filter(\"levenshtein_short = 0\")\\\n",
    "    .dropDuplicates(['prinx'])\n",
    "perfectShort = perfectShort.withColumn(\"match_type\", udf_matchtype(\"levenshtein\"))\n",
    "perfectShort = perfectShort.select(F.col(\"prinx\"), F.col(\"query_address\"), F.col(\"uprn\").alias(\"matched_uprn\"), F.col(\"target_address\").alias(\"matched_address\"), \"match_type\")\n",
    "\n",
    "\n",
    "perfectMedium = cross_compare\\\n",
    "    .join(perfectFull, \"prinx\", \"left_anti\")\\\n",
    "    .join(perfectShort, \"prinx\", \"left_anti\")\\\n",
    "    .filter(\"levenshtein_medium = 0\")\\\n",
    "    .dropDuplicates(['prinx'])\n",
    "perfectMedium = perfectMedium.withColumn(\"match_type\", udf_matchtype(\"levenshtein\"))\n",
    "perfectMedium = perfectMedium.select(F.col(\"prinx\"), F.col(\"query_address\"), F.col(\"uprn\").alias(\"matched_uprn\"), F.col(\"target_address\").alias(\"matched_address\"), \"match_type\")\n",
    "\n",
    "\n",
    "perfect10char = cross_compare\\\n",
    "    .join(perfectFull, \"prinx\", \"left_anti\")\\\n",
    "    .join(perfectShort, \"prinx\", \"left_anti\")\\\n",
    "    .join(perfectMedium, \"prinx\", \"left_anti\")\\\n",
    "    .filter(\"levenshtein_10char = 0\").dropDuplicates(['prinx'])\n",
    "perfect10char = perfect10char.withColumn(\"match_type\", udf_matchtype(\"levenshtein\"))\n",
    "perfect10char = perfect10char.select(F.col(\"prinx\"), F.col(\"query_address\"), F.col(\"uprn\").alias(\"matched_uprn\"), F.col(\"target_address\").alias(\"matched_address\"), \"match_type\")\n",
    "\n",
    "# put all 'perfect' together\n",
    "perfectMatch = perfectFull.union(perfectShort).union(perfectMedium).union(perfect10char)\n",
    "perfectMatch = perfectMatch.withColumn(\"round\", F.lit(\"round 0\"))\n",
    "\n",
    "perfectMatch.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROUND 1 - now look at imperfect with same postcode\n",
    "\n",
    "cross_compare = cross_compare\\\n",
    "    .join(perfectMatch, \"prinx\", \"left_anti\")\n",
    "\n",
    "window = Window.partitionBy('prinx').orderBy(\"levenshtein\")\n",
    "\n",
    "bestMatch_round1 = cross_compare.filter(\"levenshtein < 3\").withColumn('rank', F.rank().over(window))\\\n",
    " .filter('rank = 1')\\\n",
    " .dropDuplicates(['prinx'])\n",
    "\n",
    "# apply function for match_type\n",
    "bestMatch_round1 = bestMatch_round1.withColumn(\"match_type\", udf_matchtype(\"levenshtein\"))\n",
    "bestMatch_round1 = bestMatch_round1.withColumn(\"round\", F.lit(\"round 1\"))\n",
    "bestMatch_round1 = bestMatch_round1.select(\"prinx\", \"query_address\", F.col(\"uprn\").alias(\"matched_uprn\"), F.col(\"target_address\").alias(\"matched_address\"), \"match_type\", \"round\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMatch_round1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROUND 2\n",
    "\n",
    "cross_compare = cross_compare.join(bestMatch_round1, \"prinx\", \"left_anti\")\n",
    "\n",
    "window = Window.partitionBy('prinx').orderBy(\"levenshtein_short\")\n",
    "\n",
    "bestMatch_round2 = cross_compare.filter(\"levenshtein_short < 3\").withColumn('rank', F.rank().over(window))\\\n",
    " .filter('rank = 1')\\\n",
    " .dropDuplicates(['prinx'])\n",
    "\n",
    "bestMatch_round2 = bestMatch_round2.withColumn(\"match_type\", udf_matchtype(\"levenshtein_short\"))\n",
    "bestMatch_round2 = bestMatch_round2.withColumn(\"round\", F.lit(\"round 2\"))\n",
    "bestMatch_round2 = bestMatch_round2.select(F.col(\"prinx\"), F.col(\"query_address\"), F.col(\"uprn\").alias(\"matched_uprn\"), F.col(\"target_address\").alias(\"matched_address\"), F.col(\"match_type\"), \"round\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMatch_round2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROUND 3\n",
    "\n",
    "# take all the unmatched, still keep same postcode, try match to line1 and line 2\n",
    "cross_compare = cross_compare.join(bestMatch_round2, \"prinx\", \"left_anti\")\n",
    "\n",
    "window = Window.partitionBy('prinx').orderBy(\"levenshtein_medium\")\n",
    "\n",
    "bestMatch_round3 = cross_compare.filter(\"levenshtein_medium < 3\").withColumn('rank', F.rank().over(window))\\\n",
    " .filter('rank = 1')\\\n",
    " .dropDuplicates(['prinx'])\n",
    "\n",
    "bestMatch_round3 = bestMatch_round3.withColumn(\"match_type\", udf_matchtype(\"levenshtein_medium\"))\n",
    "bestMatch_round3 = bestMatch_round3.withColumn(\"round\", F.lit(\"round 3\"))\n",
    "bestMatch_round3 = bestMatch_round3.select(F.col(\"prinx\"), F.col(\"query_address\"), F.col(\"uprn\").alias(\"matched_uprn\"), F.col(\"target_address\").alias(\"matched_address\"), F.col(\"match_type\"), \"round\")\n",
    "\n",
    "## Prepare rounds 4 to 7: take all the unmatched, allow mismatched postcodes\n",
    "cross_with_any_postcode = query_concat\\\n",
    "    .join(perfectMatch, \"prinx\", \"left_anti\")\\\n",
    "    .join(bestMatch_round1, \"prinx\", \"left_anti\")\\\n",
    "    .join(bestMatch_round2, \"prinx\", \"left_anti\")\\\n",
    "    .join(bestMatch_round3, \"prinx\", \"left_anti\")\\\n",
    "    .crossJoin(target_concat)\n",
    "\n",
    "cross_compare = cross_with_any_postcode.withColumn(\"levenshtein\", F.levenshtein(F.col(\"query_address\"), F.col(\"target_address\")))\n",
    "cross_compare = cross_compare.withColumn(\"levenshtein_short\", F.levenshtein(F.col(\"query_address\"), F.col(\"target_address_short\")))\n",
    "cross_compare = cross_compare.withColumn(\"levenshtein_medium\", F.levenshtein(F.col(\"query_address\"), F.col(\"target_address_medium\")))\n",
    "\n",
    "cross_compare = cross_compare.filter(\"levenshtein<5 or levenshtein_short<5 or levenshtein_medium<5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMatch_round3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROUND 4\n",
    "\n",
    "window = Window.partitionBy('prinx').orderBy(\"levenshtein\")\n",
    "\n",
    "bestMatch_round4 = cross_compare.filter(\"levenshtein < 5\").withColumn('rank', F.rank().over(window))\\\n",
    " .filter('rank = 1')\\\n",
    " .dropDuplicates(['prinx'])\n",
    "\n",
    "bestMatch_round4 = bestMatch_round4.withColumn(\"match_type\", F.lit(\"imperfect match\"))\n",
    "bestMatch_round4 = bestMatch_round4.withColumn(\"round\", F.lit(\"round 4\"))\n",
    "bestMatch_round4 = bestMatch_round4.select(F.col(\"prinx\"), F.col(\"query_address\"), F.col(\"uprn\").alias(\"matched_uprn\"), F.col(\"target_address\").alias(\"matched_address\"), F.col(\"match_type\"), \"round\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMatch_round4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROUND 5\n",
    "\n",
    "# take all the unmatched, allow mismatched postcodes, match to line 1 and postcode only\n",
    "cross_compare = cross_compare.join(bestMatch_round4, \"prinx\", \"left_anti\")\n",
    "\n",
    "window = Window.partitionBy('prinx').orderBy(\"levenshtein_short\")\n",
    "\n",
    "bestMatch_round5 = cross_compare.filter(\"levenshtein_short < 5\").withColumn('rank', F.rank().over(window))\\\n",
    " .filter('rank = 1')\\\n",
    " .dropDuplicates(['prinx'])\n",
    "\n",
    "bestMatch_round5 = bestMatch_round5.withColumn(\"match_type\", F.lit(\"imperfect match\"))\n",
    "bestMatch_round5 = bestMatch_round5.withColumn(\"round\", F.lit(\"round 5\"))\n",
    "bestMatch_round5 = bestMatch_round5.select(F.col(\"prinx\"), F.col(\"query_address\"), F.col(\"uprn\").alias(\"matched_uprn\"), F.col(\"target_address\").alias(\"matched_address\"), F.col(\"match_type\"), \"round\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMatch_round5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROUND 6\n",
    "\n",
    "# take all the unmatched, allow mismatched postcodes, match to line 1 and 2 and postcode\n",
    "cross_compare = cross_compare.join(bestMatch_round5, \"prinx\", \"left_anti\")\n",
    "\n",
    "window = Window.partitionBy('prinx').orderBy(\"levenshtein_medium\")\n",
    "\n",
    "# logger.info('sort and filter')\n",
    "bestMatch_round6 = cross_compare.filter(\"levenshtein_medium < 5\").withColumn('rank', F.rank().over(window))\\\n",
    " .filter('rank = 1')\\\n",
    " .dropDuplicates(['prinx'])\n",
    "\n",
    "bestMatch_round6 = bestMatch_round6.withColumn(\"match_type\", F.lit(\"imperfect match\"))\n",
    "bestMatch_round6 = bestMatch_round6.withColumn(\"round\", F.lit(\"round 6\"))\n",
    "bestMatch_round6 = bestMatch_round6.select(F.col(\"prinx\"), F.col(\"query_address\"), F.col(\"uprn\").alias(\"matched_uprn\"), F.col(\"target_address\").alias(\"matched_address\"), F.col(\"match_type\"), \"round\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMatch_round6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROUND 7 - last chance\n",
    "\n",
    "# take all the rest and mark as unmatched\n",
    "unmatched = query_concat\\\n",
    "    .join(perfectMatch, \"prinx\", \"left_anti\")\\\n",
    "    .join(bestMatch_round1, \"prinx\", \"left_anti\")\\\n",
    "    .join(bestMatch_round2, \"prinx\", \"left_anti\")\\\n",
    "    .join(bestMatch_round3, \"prinx\", \"left_anti\")\\\n",
    "    .join(bestMatch_round4, \"prinx\", \"left_anti\")\\\n",
    "    .join(bestMatch_round5, \"prinx\", \"left_anti\")\\\n",
    "    .join(bestMatch_round6, \"prinx\", \"left_anti\")\n",
    "\n",
    "lastChanceCompare = unmatched.crossJoin(target_concat)\n",
    "cross_compare = lastChanceCompare.withColumn(\"levenshtein\", F.levenshtein(F.col(\"query_address\"), F.col(\"target_address\")))\n",
    "\n",
    "window = Window.partitionBy('prinx').orderBy(\"levenshtein\")\n",
    "\n",
    "bestMatch_lastChance = cross_compare.withColumn('rank', F.rank().over(window))\\\n",
    " .filter('rank = 1')\\\n",
    " .dropDuplicates(['prinx'])\n",
    "\n",
    "bestMatch_lastChance = bestMatch_lastChance.withColumn(\"match_type\", udf_matchtype(\"levenshtein\"))\n",
    "bestMatch_lastChance = bestMatch_lastChance.withColumn(\"round\", F.lit(\"last chance\"))\n",
    "bestMatch_lastChance = bestMatch_lastChance.select(F.col(\"prinx\"), F.col(\"query_address\"), F.col(\"uprn\").alias(\"matched_uprn\"), F.col(\"target_address\").alias(\"matched_address\"), F.col(\"match_type\"), \"round\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMatch_lastChance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PUT RESULTS OF ALL ROUNDS TOGETHER\n",
    "\n",
    "all_best_match = perfectMatch.union(bestMatch_round1).union(bestMatch_round2).union(bestMatch_round3).union(bestMatch_round4).union(bestMatch_round5).union(bestMatch_round6).union(bestMatch_lastChance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JOIN MATCH RESULTS WITH INITIAL DATASET\n",
    "\n",
    "matchingResults = query_addresses.join(all_best_match, \"prinx\", \"left\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchingResults.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE TO S3\n",
    "\n",
    "resultDataFrame = DynamicFrame.fromDF(matchingResults, glueContext, \"resultDataFrame\")\n",
    "\n",
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=resultDataFrame,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": target_destination, \"partitionKeys\": PARTITION_KEYS},\n",
    "    transformation_ctx=\"parquetData\")\n",
    "\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
