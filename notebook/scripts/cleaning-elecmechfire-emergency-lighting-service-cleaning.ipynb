{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('housing-repairs-emergency-lighting-service-cleaning')\n",
    "\n",
    "sys.argv.append('--source_catalog_database')\n",
    "sys.argv.append('housing-repairs-raw-zone')\n",
    "\n",
    "sys.argv.append('--source_catalog_table')\n",
    "sys.argv.append('housing_repairs_emergency_lighting_servicing')\n",
    "\n",
    "sys.argv.append('--cleaned_repairs_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-refined-zone/housing-repairs/repairs-electrical-mechanical-fire/emergency-lighting-servicing/cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pyspark.sql.functions as F\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_glue_env_var, get_latest_partitions, PARTITION_KEYS\n",
    "from repairs_cleaning_helpers import udf_map_repair_priority, clean_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import boto3\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "PARTITION_KEYS = ['import_year', 'import_month', 'import_day', 'import_date']\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "def get_secret(logger, secret_name, region_name):\n",
    "    session = boto3.session.Session()\n",
    "\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    get_secret_value_response = client.get_secret_value(\n",
    "        SecretId=secret_name\n",
    "    )\n",
    "\n",
    "    if 'SecretString' in get_secret_value_response:\n",
    "        return get_secret_value_response['SecretString']\n",
    "    else:\n",
    "        return get_secret_value_response['SecretBinary'].decode('ascii')\n",
    "\n",
    "def add_timestamp_column(data_frame):\n",
    "    now = datetime.datetime.now()\n",
    "    return data_frame.withColumn('import_timestamp', f.lit(str(now.timestamp())))\n",
    "\n",
    "def add_import_time_columns(data_frame):\n",
    "    now = datetime.datetime.now()\n",
    "    importYear = str(now.year)\n",
    "    importMonth = str(now.month).zfill(2)\n",
    "    importDay = str(now.day).zfill(2)\n",
    "    importDate = importYear + importMonth + importDay\n",
    "\n",
    "    data_frame = data_frame.withColumn(\n",
    "        'import_datetime', f.current_timestamp())\n",
    "    data_frame = data_frame.withColumn(\n",
    "        'import_timestamp', f.lit(str(now.timestamp())))\n",
    "    data_frame = data_frame.withColumn('import_year', f.lit(importYear))\n",
    "    data_frame = data_frame.withColumn('import_month', f.lit(importMonth))\n",
    "    data_frame = data_frame.withColumn('import_day', f.lit(importDay))\n",
    "    data_frame = data_frame.withColumn('import_date', f.lit(importDate))\n",
    "    return data_frame\n",
    "\n",
    "def convert_pandas_df_to_spark_dynamic_df(sql_context, panadas_df):\n",
    "    # Convert to SparkDynamicDataFrame\n",
    "    spark_df = sql_context.createDataFrame(panadas_df)\n",
    "    spark_df = spark_df.coalesce(1)\n",
    "    spark_df = add_import_time_columns(spark_df)\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "def get_s3_subfolders(s3_client, bucket_name, prefix):\n",
    "    there_are_more_objects_in_the_bucket_to_fetch = True\n",
    "    folders = []\n",
    "    continuation_token = {}\n",
    "    while there_are_more_objects_in_the_bucket_to_fetch:\n",
    "        list_objects_response = s3_client.list_objects_v2(\n",
    "            Bucket=bucket_name,\n",
    "            Delimiter='/',\n",
    "            Prefix=prefix,\n",
    "            **continuation_token\n",
    "        )\n",
    "\n",
    "        folders.extend(x['Prefix']\n",
    "                       for x in list_objects_response.get('CommonPrefixes', []))\n",
    "        there_are_more_objects_in_the_bucket_to_fetch = list_objects_response['IsTruncated']\n",
    "        continuation_token['ContinuationToken'] = list_objects_response.get(\n",
    "            'NextContinuationToken')\n",
    "\n",
    "    return set(folders)\n",
    "\n",
    "def get_latest_partitions(dfa):\n",
    "    dfa = dfa.where(f.col('import_year') == dfa.select(\n",
    "        f.max('import_year')).first()[0])\n",
    "    dfa = dfa.where(f.col('import_month') == dfa.select(\n",
    "        f.max('import_month')).first()[0])\n",
    "    dfa = dfa.where(f.col('import_day') == dfa.select(\n",
    "        f.max('import_day')).first()[0])\n",
    "    return dfa\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "\n",
    "def map_repair_priority(code):\n",
    "    if code == 'Immediate':\n",
    "        return 1\n",
    "    elif code == 'Emergency':\n",
    "        return 2\n",
    "    elif code == 'Urgent':\n",
    "        return 3\n",
    "    elif code == 'Normal':\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# # convert to a UDF Function by passing in the function and the return type of function (string in this case)\n",
    "udf_map_repair_priority = F.udf(map_repair_priority, StringType())\n",
    "\n",
    "\n",
    "def clean_column_names(df):\n",
    "    # remove full stops from column names\n",
    "    df = df.select([F.col(\"`{0}`\".format(c)).alias(\n",
    "        c.replace('.', '')) for c in df.columns])\n",
    "    # remove trialing underscores\n",
    "    df = df.select([F.col(col).alias(re.sub(\"_$\", \"\", col))\n",
    "                   for col in df.columns])\n",
    "    # lowercase and remove double underscores\n",
    "    df2 = df.select([F.col(col).alias(\n",
    "        re.sub(\"[^0-9a-zA-Z$]+\", \"_\", col.lower())) for col in df.columns])\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "source_catalog_table = get_glue_env_var('source_catalog_table', '')\n",
    "cleaned_repairs_s3_bucket_target = get_glue_env_var('cleaned_repairs_s3_bucket_target', '')\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = glueContext.create_dynamic_frame.from_catalog(\n",
    "    name_space=source_catalog_database,\n",
    "    table_name=source_catalog_table,\n",
    ")\n",
    "\n",
    "df = source_data.toDF()\n",
    "df = get_latest_partitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df3.show(n=50, truncate=100, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = clean_column_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "outputs": [],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2\n",
    "\n",
    "# rename column names to reflect harmonised column names\n",
    "df3 = df3.withColumnRenamed('requested_by', 'operative') \\\n",
    "    .withColumnRenamed('address', 'property_address') \\\n",
    "    .withColumnRenamed('description', 'description_of_work') \\\n",
    "    .withColumnRenamed('priority_code', 'work_priority_description') \\\n",
    "    .withColumnRenamed('temp_order_number', 'temp_order_number_full') \\\n",
    "    .withColumnRenamed('cost', 'order_value')\n",
    "\n",
    "df3 = df3.withColumn('date', F.to_timestamp('date', \"yyyy-MM-dd\")).withColumnRenamed('date', 'datetime_raised')\n",
    "df3 = df3.withColumn('data_source', F.lit('ElecMechFire - Emergency Lighting Service'))\n",
    "df3 = df3.withColumn('work_priority_priority_code', udf_map_repair_priority('work_priority_description'))\n",
    "df3 = df3.withColumn('status_of_completed_y_n', F.when(df3['status_of_completed_y_n']=='Y', 'Completed').otherwise(''))\\\n",
    "    .withColumnRenamed('status_of_completed_y_n', 'order_status')\n",
    "\n",
    "# only keep relevant columns\n",
    "df3 = df3[[\n",
    "    'datetime_raised',\n",
    "    'operative',\n",
    "    'property_address',\n",
    "    'description_of_work',\n",
    "    'work_priority_description',\n",
    "    'temp_order_number_full',\n",
    "    'order_value',\n",
    "    'order_status',\n",
    "    'data_source',\n",
    "    'import_datetime',\n",
    "    'import_timestamp',\n",
    "    'import_year',\n",
    "    'import_month',\n",
    "    'import_day',\n",
    "    'import_date'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "outputs": [],
   "source": [
    "df3.printSchema()\n",
    "df3.show(n=50, truncate=100, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDataframe = DynamicFrame.fromDF(df3, glueContext, \"cleanedDataframe\")\n",
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=cleanedDataframe,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": cleaned_repairs_s3_bucket_target,\"partitionKeys\": PARTITION_KEYS},\n",
    "    transformation_ctx=\"parquetData\")\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
