{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('cleaning-repairs-herts-heritage')\n",
    "\n",
    "sys.argv.append('--source_catalog_database')\n",
    "sys.argv.append('housing-repairs-raw-zone')\n",
    "\n",
    "sys.argv.append('--source_catalog_table')\n",
    "sys.argv.append('housing_repairs_repairs_herts_heritage')\n",
    "\n",
    "sys.argv.append('--cleaned_repairs_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-refined-zone/housing-repairs/repairs-herts-heritage/cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, max\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "#from helpers import get_glue_env_var, PARTITION_KEYS\n",
    "#from repairs_cleaning_helpers import udf_map_repair_priority, clean_column_names\n",
    "\n",
    "def getLatestPartitions(dfa):\n",
    "   dfa = dfa.where(col('import_year') == dfa.select(max('import_year')).first()[0])\n",
    "   dfa = dfa.where(col('import_month') == dfa.select(max('import_month')).first()[0])\n",
    "   dfa = dfa.where(col('import_day') == dfa.select(max('import_day')).first()[0])\n",
    "   return dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.utils import getResolvedOptions\n",
    "import datetime\n",
    "import boto3\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "PARTITION_KEYS = ['import_year', 'import_month', 'import_day', 'import_date']\n",
    "\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "\n",
    "def get_secret(logger, secret_name, region_name):\n",
    "    session = boto3.session.Session()\n",
    "\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    get_secret_value_response = client.get_secret_value(\n",
    "        SecretId=secret_name\n",
    "    )\n",
    "\n",
    "    if 'SecretString' in get_secret_value_response:\n",
    "        return get_secret_value_response['SecretString']\n",
    "    else:\n",
    "        return get_secret_value_response['SecretBinary'].decode('ascii')\n",
    "\n",
    "def add_timestamp_column(data_frame):\n",
    "    now = datetime.datetime.now()\n",
    "    return data_frame.withColumn('import_timestamp', f.lit(str(now.timestamp())))\n",
    "\n",
    "def add_import_time_columns(data_frame):\n",
    "    now = datetime.datetime.now()\n",
    "    importYear = str(now.year)\n",
    "    importMonth = str(now.month).zfill(2)\n",
    "    importDay = str(now.day).zfill(2)\n",
    "    importDate = importYear + importMonth + importDay\n",
    "\n",
    "    data_frame = data_frame.withColumn('import_datetime', f.current_timestamp())\n",
    "    data_frame = data_frame.withColumn('import_timestamp', f.lit(str(now.timestamp())))\n",
    "    data_frame = data_frame.withColumn('import_year', f.lit(importYear))\n",
    "    data_frame = data_frame.withColumn('import_month', f.lit(importMonth))\n",
    "    data_frame = data_frame.withColumn('import_day', f.lit(importDay))\n",
    "    data_frame = data_frame.withColumn('import_date', f.lit(importDate))\n",
    "    return data_frame\n",
    "\n",
    "def convert_pandas_df_to_spark_dynamic_df(sql_context, panadas_df):\n",
    "    # Convert to SparkDynamicDataFrame\n",
    "    spark_df = sql_context.createDataFrame(panadas_df)\n",
    "    spark_df = spark_df.coalesce(1)\n",
    "    spark_df = add_import_time_columns(spark_df)\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "def get_s3_subfolders(s3_client, bucket_name, prefix):\n",
    "  there_are_more_objects_in_the_bucket_to_fetch = True\n",
    "  folders = []\n",
    "  continuation_token = {}\n",
    "  while there_are_more_objects_in_the_bucket_to_fetch:\n",
    "    list_objects_response = s3_client.list_objects_v2(\n",
    "      Bucket=bucket_name,\n",
    "      Delimiter='/',\n",
    "      Prefix=prefix,\n",
    "      **continuation_token\n",
    "    )\n",
    "\n",
    "    folders.extend(x['Prefix'] for x in list_objects_response.get('CommonPrefixes', []))\n",
    "    there_are_more_objects_in_the_bucket_to_fetch = list_objects_response['IsTruncated']\n",
    "    continuation_token['ContinuationToken'] = list_objects_response.get('NextContinuationToken')\n",
    "\n",
    "  return set(folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "def map_repair_priority(code):\n",
    "    if code == 'Immediate':\n",
    "        return 1\n",
    "    elif code == 'Emergency':\n",
    "        return 2\n",
    "    elif code == 'Urgent':\n",
    "        return 3\n",
    "    elif code == 'Normal':\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# convert to a UDF Function by passing in the function and the return type of function (string in this case)\n",
    "udf_map_repair_priority = F.udf(map_repair_priority, StringType())\n",
    "\n",
    "def clean_column_names(df):\n",
    "    # remove trialing underscores\n",
    "    df = df.select([F.col(col).alias(re.sub(\"_$\", \"\", col)) for col in df.columns])\n",
    "    # lowercase and remove double underscores\n",
    "    df2 = df.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\", \"_\", col.lower())) for col in df.columns])\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "source_catalog_table = get_glue_env_var('source_catalog_table', '')\n",
    "cleaned_repairs_s3_bucket_target = get_glue_env_var('cleaned_repairs_s3_bucket_target', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = glueContext.create_dynamic_frame.from_catalog(\n",
    "    name_space=source_catalog_database,\n",
    "    table_name=source_catalog_table,\n",
    ")\n",
    "df = source_data.toDF()\n",
    "df = getLatestPartitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = clean_column_names(df)\n",
    "\n",
    "df2 = df2.withColumn('time_stamp', F.to_timestamp(\"time_stamp\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "df2 = df2.withColumn('data_source', F.lit('Heritage'))\n",
    "\n",
    "df2 = df2.withColumnRenamed('time_stamp', 'timestamp') \\\n",
    "    .withColumnRenamed('notes_and_information', 'notes') \\\n",
    "    .withColumnRenamed('contact_information_for_access', 'phone_1') \\\n",
    "    .withColumnRenamed('priority_code', 'work_priority_description') \\\n",
    "    .withColumnRenamed('email_address', 'email_staff') \\\n",
    "    .withColumnRenamed('temporary_order_date', 'temp_order_number_date') \\\n",
    "    .withColumnRenamed('temporary_order_number__time_', 'temp_order_number_time') \\\n",
    "    .withColumnRenamed('STATUS', 'order_status') \\\n",
    "    .withColumnRenamed('status_notes', 'order_status_notes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDataframe = DynamicFrame.fromDF(df2, glueContext, \"cleanedDataframe\")\n",
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=cleanedDataframe,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": cleaned_repairs_s3_bucket_target,\"partitionKeys\": PARTITION_KEYS},\n",
    "    transformation_ctx=\"parquetData\")\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
