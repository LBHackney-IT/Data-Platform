{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('address-cleaning')\n",
    "\n",
    "sys.argv.append('--source_catalog_database')\n",
    "sys.argv.append('housing-repairs-raw-zone')\n",
    "\n",
    "sys.argv.append('--source_catalog_table')\n",
    "sys.argv.append('housing_repairs_repairs_dlo')\n",
    "\n",
    "sys.argv.append('--cleaned_repairs_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-refined-zone/housing/repairs-dlo/cleaned-repairs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, HashingTF, MinHashLSH\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, trim, when, max\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "def getLatestPartitions(dfa):\n",
    "   dfa = dfa.where(col('import_year') == dfa.select(max('import_year')).first()[0])\n",
    "   dfa = dfa.where(col('import_month') == dfa.select(max('import_month')).first()[0])\n",
    "   dfa = dfa.where(col('import_day') == dfa.select(max('import_day')).first()[0])\n",
    "   return dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "source_catalog_table    = get_glue_env_var('source_catalog_table', '')\n",
    "cleaned_repairs_s3_bucket_target = get_glue_env_var('cleaned_repairs_s3_bucket_target', '')\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "logger.info('Fetch Source Data')\n",
    "\n",
    "source_data = glueContext.create_dynamic_frame.from_catalog(\n",
    "    name_space=source_catalog_database,\n",
    "    table_name=source_catalog_table,\n",
    ") \n",
    "\n",
    "df = source_data.toDF()\n",
    "df = getLatestPartitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.toDF(*[c.lower().replace(' ', '_') for c in df.columns])\n",
    "df2 = df.toDF(*[c.lower().replace('-', '_') for c in df.columns])\n",
    "df2 = df.toDF(*[c.lower().replace('__', '_') for c in df.columns])\n",
    "print(df2)\n",
    "\n",
    "logger.info('convert timestamp column to a datetime field type')\n",
    "df2 = df2.withColumn('timestamp', F.to_timestamp(\"timestamp\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "# df = df.withColumn('address', F.col('Address of repair'))\n",
    "# df = df.withColumn('address', F.regexp_replace('address', '\\n', ' '))\n",
    "\n",
    "df2 = df2.withColumn('name_of_resident', F.initcap(F.col('name_of_resident')))\n",
    "df2 = df2.withColumn('data_source', F.lit('DLO'))\n",
    "\n",
    "df2 = df2.withColumnRenamed('name_of_resident', 'name_full') \\\n",
    "    .withColumnRenamed('job_description', 'description_of_work') \\\n",
    "    .withColumnRenamed('which_trade_needs_to_respond_to_repair?', 'trade_description') \\\n",
    "    .withColumnRenamed('what_is_the_priority_for_the_repair?', 'work_priority_description') \\\n",
    "    .withColumnRenamed('date_of_appointment', 'appointment_date') \\\n",
    "    .withColumnRenamed('if_there_is_a_cautionary_contact_alert,_what_is_the_nature_of_it?', 'alert_regarding_person_notes') \\\n",
    "    .withColumnRenamed('if_yes,_what_vulnerabilities_do_they_have?', 'vulnerability_notes') \\\n",
    "    .withColumnRenamed('postcode_of_property', 'postal_code_raw') \\\n",
    "    .withColumnRenamed('planners_to_allocate_to_operatives', 'operative') \\\n",
    "    .withColumnRenamed('does_the_resident_have_any_vulnerabilities?', 'vulnerability_flag') \\\n",
    "    .withColumnRenamed('is_there_a_cautionary_contact_alert_at_this_address?', 'alert_regarding_person') \\\n",
    "    .withColumnRenamed('planners_to_allocate_to_operatives', 'operative') \\\n",
    "    .withColumnRenamed('make_a_note_if_the_resident_is_reporting_any_coronavirus_symptoms_in_the_household_and_advise_residents_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing_', 'covid_notes') \\\n",
    "    .withColumnRenamed('have_you_read_the_coronavirus_statement_to_the_resident?_please_advise_the_resident_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing_', 'covid_statement_given') \\\n",
    "    .withColumnRenamed('uh_property_reference', 'property_reference_uh') \\\n",
    "    .withColumnRenamed('housing_status:_is_the_resident_a..._select_as_many_as_apply_', 'property_address_type') \\\n",
    "    .withColumnRenamed('is_the_job_a_recharge_or_sus_recharge?', 'recharge') \\\n",
    "    .withColumnRenamed('form_reference_-_do_not_alter', 'form_ref') \\\n",
    "    .withColumnRenamed('phone_number_of_resident', 'phone_1') \\\n",
    "    .withColumnRenamed('address_of_repair', 'property_address') \\\n",
    "    .withColumnRenamed('time_of_appointment', 'appointment_time') \\\n",
    "    .withColumnRenamed('planners_notes', 'notes') \\\n",
    "    .withColumnRenamed('email_address', 'email_staff') \\\n",
    "    .withColumnRenamed('uh_phone_number_1', 'phone_2') \\\n",
    "    .withColumnRenamed('uh_phone_number_2', 'phone_3') \\\n",
    "    .withColumnRenamed('timestamp', 'datetime_raised') \\\n",
    "\n",
    "\n",
    "temp = df2[['description_of_work', 'property_address_type']]\n",
    "temp.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_repair_priority(code):\n",
    "    if code == 'Immediate (2hr response)':\n",
    "        return 1\n",
    "    elif code == 'Emergency (24hrs)':\n",
    "        return 2\n",
    "    elif code == 'Urgent (5 working days)':\n",
    "        return 3\n",
    "    elif code == 'Normal (21 working days)':\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# # convert to a UDF Function by passing in the function and the return type of function (string in this case)\n",
    "udf_map_repair_priority = F.udf(map_repair_priority, StringType())\n",
    "# apply function\n",
    "df2 = df2.withColumn('work_priority_code', udf_map_repair_priority('work_priority_description'))\n",
    "temp = df2[['work_priority_description', 'work_priority_code']]\n",
    "temp.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDataframe = DynamicFrame.fromDF(df2, glueContext, \"cleanedDataframe\")\n",
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=cleanedDataframe,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": cleaned_repairs_s3_bucket_target,\"partitionKeys\": [\"import_year\", \"import_month\", \"import_day\", \"import_date\"]},\n",
    "    transformation_ctx=\"parquetData\")\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
