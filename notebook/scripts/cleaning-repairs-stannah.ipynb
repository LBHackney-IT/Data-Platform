{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('address-cleaning')\n",
    "\n",
    "sys.argv.append('--source_catalog_database')\n",
    "sys.argv.append('housing-repairs-raw-zone')\n",
    "\n",
    "sys.argv.append('--source_catalog_table')\n",
    "sys.argv.append('housing_repairs_repairs_stannah')\n",
    "\n",
    "sys.argv.append('--cleaned_repairs_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-refined-zone/housing-repairs/repairs-stannah/cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, HashingTF, MinHashLSH\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, trim, when, max, trim\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import re\n",
    "\n",
    "#TODO get these functions from helpers\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "def getLatestPartitions(dfa):\n",
    "   dfa = dfa.where(col('import_year') == dfa.select(max('import_year')).first()[0])\n",
    "   dfa = dfa.where(col('import_month') == dfa.select(max('import_month')).first()[0])\n",
    "   dfa = dfa.where(col('import_day') == dfa.select(max('import_day')).first()[0])\n",
    "   return dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "source_catalog_table    = get_glue_env_var('source_catalog_table', '')\n",
    "cleaned_repairs_s3_bucket_target = get_glue_env_var('cleaned_repairs_s3_bucket_target', '')\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "logger.info('Fetch Source Data')\n",
    "\n",
    "source_data = glueContext.create_dynamic_frame.from_catalog(\n",
    "    name_space=source_catalog_database,\n",
    "    table_name=source_catalog_table,\n",
    "#     push_down_predicate=\"import_date==max(import_date)\"\n",
    ") \n",
    "\n",
    "df = source_data.toDF()\n",
    "df = getLatestPartitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select([F.col(col).alias(re.sub(\"_$\", \"\", col)) for col in df.columns])\n",
    "df2 = df.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\", \"_\", col.lower())) for col in df.columns])\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('convert timestamp and date columns to datetime / date field types')\n",
    "df2 = df2.withColumn('timestamp', F.to_timestamp(\"timestamp\", \"dd/MM/yyyy HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new data source column to specify which repairs sheet the repair came from\n",
    "df2 = df2.withColumn('data_source', F.lit('Stannah'))\n",
    "\n",
    "# rename column names\n",
    "df2 = df2.withColumnRenamed('email_address', 'email_staff') \\\n",
    "    .withColumnRenamed('temporary_order_number', 'temp_order_number_full') \\\n",
    "    .withColumnRenamed('sors_frequent_use_only', 'sor') \\\n",
    "    .withColumnRenamed('priority_code', 'work_priority_description') \\\n",
    "    .withColumnRenamed('notes_and_information', 'notes') \\\n",
    "    .withColumnRenamed('value_costs', 'order_value')\n",
    "\n",
    "# drop columns not needed\n",
    "\n",
    "df2 = df2.drop(df2.column10)\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDataframe = DynamicFrame.fromDF(df2, glueContext, \"cleanedDataframe\")\n",
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=cleanedDataframe,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": cleaned_repairs_s3_bucket_target,\"partitionKeys\": [\"import_year\", \"import_month\", \"import_day\", \"import_date\"]},\n",
    "    transformation_ctx=\"parquetData\")\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
