{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO I am not sure if we need to initiate a SparkContext or if the notebook takes care of this?\n",
    "\n",
    "# Alternative to Databricks display function.\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").config(\"spark.sql.broadcastTimeout\", \"-1\").appName('repairs-cleaning').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repairs file location and type\n",
    "file_location = \"repairs_dlo.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "multiline = \"true\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "      .option(\"inferSchema\", infer_schema) \\\n",
    "      .option(\"header\", first_row_is_header) \\\n",
    "      .option(\"multiLine\", multiline) \\\n",
    "      .option(\"sep\", delimiter) \\\n",
    "      .load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert column names to lower case and replace spaces with underscores\n",
    "df2 = df.toDF(*[c.lower().replace(' ', '_') for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp column to a datetime field type\n",
    "df2 = df2.withColumn('timestamp', F.to_timestamp(\"timestamp\", \"dd/MM/yyyy HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove text from timestamp column\n",
    "\n",
    "# def get_mean_timestamp(timestamp):\n",
    "#     if len(timestamp) <16:\n",
    "#         return '01/01/1970 00:00:00'\n",
    "\n",
    "\n",
    "# # # convert to a UDF Function by passing in the function and the return type of function (string in this case)\n",
    "# udf_get_mean_timestamp = F.udf(get_mean_timestamp, TimestampType())\n",
    "# # apply function\n",
    "# df2 = df2.withColumn(\"timestamp\", udf_get_mean_timestamp(\"timestamp\"))\n",
    "# df2.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows without a valid date\n",
    "df2 = df2.na.drop(subset=[\"timestamp\"])\n",
    "\n",
    "# remove \\n from address field\n",
    "df = df.withColumn('address', F.col('Address of repair'))\n",
    "df = df.withColumn('address', F.regexp_replace('address', '\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert name field to Title case\n",
    "df2 = df2.withColumn('name_of_resident', F.initcap(F.col('name_of_resident')))\n",
    "\n",
    "# add data source column\n",
    "df2 = df2.withColumn('data_source', F.lit('DLO'))\n",
    "\n",
    "# rename columns\n",
    "df2 = df2.withColumnRenamed('name_of_resident', 'name_full') \\\n",
    "    .withColumnRenamed('job_description', 'description_of_work') \\\n",
    "    .withColumnRenamed('which_trade_needs_to_respond_to_repair?', 'trade_description') \\\n",
    "    .withColumnRenamed('what_is_the_priority_for_the_repair?', 'work_priority_description') \\\n",
    "    .withColumnRenamed('date_of_appointment', 'appointment_date') \\\n",
    "    .withColumnRenamed('if_there_is_a_cautionary_contact_alert,_what_is_the_nature_of_it?', 'alert_regarding_person_notes') \\\n",
    "    .withColumnRenamed('if_yes,_what_vulnerabilities_do_they_have?', 'vulnerability_notes') \\\n",
    "    .withColumnRenamed('postcode_of_property', 'postal_code_raw') \\\n",
    "    .withColumnRenamed('planners_to_allocate_to_operatives', 'operative') \\\n",
    "    .withColumnRenamed('does_the_resident_have_any_vulnerabilities?', 'vulnerability_flag') \\\n",
    "    .withColumnRenamed('is_there_a_cautionary_contact_alert_at_this_address?', 'alert_regarding_person') \\\n",
    "    .withColumnRenamed('planners_to_allocate_to_operatives', 'operative') \\\n",
    "    .withColumnRenamed('make_a_note_if_the_resident_is_reporting_any_coronavirus_symptoms_in_the_household_and_advise_residents_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing_', 'covid_notes') \\\n",
    "    .withColumnRenamed('have_you_read_the_coronavirus_statement_to_the_resident?_please_advise_the_resident_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing_', 'covid_statement_given') \\\n",
    "    .withColumnRenamed('uh_property_reference', 'property_reference_uh') \\\n",
    "    .withColumnRenamed('housing_status:_is_the_resident_a..._(select_as_many_as_apply)', 'property_address_type') \\\n",
    "    .withColumnRenamed('is_the_job_a_recharge_or_sus_recharge?', 'recharge') \\\n",
    "    .withColumnRenamed('form_reference_-_do_not_alter', 'form_ref') \\\n",
    "    .withColumnRenamed('phone_number_of_resident', 'phone_1') \\\n",
    "    .withColumnRenamed('address_of_repair', 'property_address') \\\n",
    "    .withColumnRenamed('phone_number_of_resident', 'phone_1') \\\n",
    "    .withColumnRenamed('planners_notes', 'notes') \\\n",
    "    .withColumnRenamed('time_of_appointment', 'appointment_time') \\\n",
    "    .withColumnRenamed('planners_notes', 'notes') \\\n",
    "    .withColumnRenamed('email_address', 'email_staff') \\\n",
    "    .withColumnRenamed('uh_phone_number_1', 'phone_2') \\\n",
    "    .withColumnRenamed('uh_phone_number_2', 'phone_3') \\\n",
    "    .withColumnRenamed('timestamp', 'datetime_raised') \\\n",
    "\n",
    "\n",
    "df2.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.toPandas()['work_priority_description'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap 'work_priority_priority_code' column\n",
    "def map_repair_priority(code):\n",
    "    if code == 'Immediate (2hr response)':\n",
    "        return 1\n",
    "    elif code == 'Emergency (24hrs)':\n",
    "        return 2\n",
    "    elif code == 'Urgent (5 working days)':\n",
    "        return 3\n",
    "    elif code == 'Normal (21 working days)':\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# # convert to a UDF Function by passing in the function and the return type of function (string in this case)\n",
    "udf_map_repair_priority = F.udf(map_repair_priority, StringType())\n",
    "# apply function\n",
    "df2 = df2.withColumn('work_priority_code', udf_map_repair_priority('work_priority_description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get UPRN data\n",
    "file_location = \"vulnerability.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "multiline = \"true\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "vp = spark.read.format(file_type) \\\n",
    "      .option(\"inferSchema\", infer_schema) \\\n",
    "      .option(\"header\", first_row_is_header) \\\n",
    "      .option(\"multiLine\", multiline) \\\n",
    "      .option(\"sep\", delimiter) \\\n",
    "      .load(file_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep certain columns\n",
    "vp = vp.select('uprn', 'ten_property_ref')\n",
    "# rename so has same name as repairs table\n",
    "vp = vp.withColumnRenamed('ten_property_ref', 'property_reference_uh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join uh prop ref to get uprn\n",
    "df2 = df2.join(vp, 'property_reference_uh', 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select and organise columns\n",
    "df2 = df2.select(['id', 'datetime_raised',\n",
    " 'operative',\n",
    " 'notes',\n",
    " 'name_full',\n",
    " 'property_address', \n",
    " 'uprn',\n",
    " 'phone_1',\n",
    " 'property_address_type',\n",
    " 'description_of_work',\n",
    " 'trade_description',\n",
    " 'work_priority_description',\n",
    " 'work_priority_code',\n",
    " 'appointment_date',\n",
    " 'appointment_time',\n",
    " 'covid_statement_given',\n",
    " 'covid_notes',\n",
    " 'recharge',\n",
    " 'alert_regarding_person',\n",
    " 'alert_regarding_person_notes',\n",
    " 'vulnerability_flag',\n",
    " 'vulnerability_notes',\n",
    " 'email_staff',\n",
    " 'postal_code_raw',\n",
    " 'phone_2',\n",
    " 'phone_3',\n",
    " 'block_name',\n",
    " 'estate_name',\n",
    " 'block_reference',\n",
    " 'estate_reference',\n",
    " 'property_reference_uh',\n",
    " 'form_ref',\n",
    " 'data_source'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.toPandas().to_csv('repairs_dlo_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
