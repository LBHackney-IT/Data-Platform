{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>11</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('James1')\n",
    "\n",
    "sys.argv.append('--source_dataset')\n",
    "sys.argv.append('s3://dataplatform-stg-landing-zone/housing/repairs-DLO/')\n",
    "\n",
    "sys.argv.append('--source_address_column')\n",
    "sys.argv.append('address_of_repair')\n",
    "\n",
    "sys.argv.append('--source_postcode_column')\n",
    "sys.argv.append('postcode_of_property')\n",
    "\n",
    "sys.argv.append('--cleaned_addresses_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-refined-zone/housing/repairs-DLO/cleanedAddresses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, HashingTF, MinHashLSH\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, trim, when\n",
    "import pyspark.sql.functions as F\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "## write into the log file with:\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "source_dataset_path = get_glue_env_var('source_dataset', '')\n",
    "source_address_column = get_glue_env_var('source_address_column', '')\n",
    "source_postcode_column = get_glue_env_var('source_postcode_column', '')\n",
    "cleaned_addresses_s3_bucket_target = get_glue_env_var('cleaned_addresses_s3_bucket_target', '')\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://localhost:8998/sessions/11 with error payload: {\"msg\":\"Session '11' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "logger.info('Fetch Source Data')\n",
    "source_dataset = glueContext.create_dynamic_frame.from_catalog(\n",
    "    name_space='dataplatform-stg-landing-zone-database',\n",
    "    table_name='housing_repairs_dlo',\n",
    "    transformation_ctx='source_dataset',\n",
    ")\n",
    "\n",
    "#(\n",
    "#    connection_type=\"s3\",\n",
    "#    format=\"parquet\",\n",
    "#    connection_options={\n",
    "#        \"paths\": [source_dataset_path],\n",
    "#        \"recurse\": True,\n",
    "#        'groupFiles': 'inPartition',\n",
    "#    },\n",
    "#    transformation_ctx=\"source_dataset\"\n",
    "#)\n",
    "\n",
    "df = source_dataset.toDF()\n",
    "source_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "\"cannot resolve '`import_year`' given input columns: [make_a_note_if_the_resident_is_reporting_any_coronavirus_symptoms_in_the_household_and_advise_residents_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing, import_timestamp, postcode_of_property, uh_phone_number_1, block_reference, Timestamp, column29, job_description, uh_phone_number_2, if_yes__what_vulnerabilities_do_they_have?, is_there_a_cautionary_contact_alert_at_this_address?, form_reference_-_do_not_alter, if_there_is_a_cautionary_contact_alert__what_is_the_nature_of_it?, time_of_appointment, id, estate_reference, planners_notes, estate_name, is_the_job_a_recharge_or_sus_recharge?, planners_to_allocate_to_operatives, have_you_read_the_coronavirus_statement_to_the_resident?_please_advise_the_resident_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing, phone_number_of_resident, date_of_appointment, what_is_the_priority_for_the_repair?, email_address, block_name, housing_status:_is_the_resident_a...__select_as_many_as_apply_, uh_property_reference, does_the_resident_have_any_vulnerabilities?, name_of_resident, which_trade_needs_to_respond_to_repair?, import_date, address_of_repair];;\\n'Project [name_of_resident#541, address_of_repair#542, postcode_of_property#558, import_date#569, import_timestamp#570, 'import_year]\\n+- LogicalRDD [Timestamp#538, planners_to_allocate_to_operatives#539, planners_notes#540, name_of_resident#541, address_of_repair#542, phone_number_of_resident#543, housing_status:_is_the_resident_a...__select_as_many_as_apply_#544, job_description#545, which_trade_needs_to_respond_to_repair?#546, what_is_the_priority_for_the_repair?#547, date_of_appointment#548, time_of_appointment#549, have_you_read_the_coronavirus_statement_to_the_resident?_please_advise_the_resident_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing#550, make_a_note_if_the_resident_is_reporting_any_coronavirus_symptoms_in_the_household_and_advise_residents_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing#551, is_the_job_a_recharge_or_sus_recharge?#552, is_there_a_cautionary_contact_alert_at_this_address?#553, if_there_is_a_cautionary_contact_alert__what_is_the_nature_of_it?#554, does_the_resident_have_any_vulnerabilities?#555, if_yes__what_vulnerabilities_do_they_have?#556, email_address#557, postcode_of_property#558, uh_phone_number_1#559, uh_phone_number_2#560, block_name#561, ... 9 more fields], false\\n\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1284, in __getitem__\n",
      "    return self.select(*item)\n",
      "  File \"/home/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1320, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/home/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/home/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`import_year`' given input columns: [make_a_note_if_the_resident_is_reporting_any_coronavirus_symptoms_in_the_household_and_advise_residents_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing, import_timestamp, postcode_of_property, uh_phone_number_1, block_reference, Timestamp, column29, job_description, uh_phone_number_2, if_yes__what_vulnerabilities_do_they_have?, is_there_a_cautionary_contact_alert_at_this_address?, form_reference_-_do_not_alter, if_there_is_a_cautionary_contact_alert__what_is_the_nature_of_it?, time_of_appointment, id, estate_reference, planners_notes, estate_name, is_the_job_a_recharge_or_sus_recharge?, planners_to_allocate_to_operatives, have_you_read_the_coronavirus_statement_to_the_resident?_please_advise_the_resident_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing, phone_number_of_resident, date_of_appointment, what_is_the_priority_for_the_repair?, email_address, block_name, housing_status:_is_the_resident_a...__select_as_many_as_apply_, uh_property_reference, does_the_resident_have_any_vulnerabilities?, name_of_resident, which_trade_needs_to_respond_to_repair?, import_date, address_of_repair];;\\n'Project [name_of_resident#541, address_of_repair#542, postcode_of_property#558, import_date#569, import_timestamp#570, 'import_year]\\n+- LogicalRDD [Timestamp#538, planners_to_allocate_to_operatives#539, planners_notes#540, name_of_resident#541, address_of_repair#542, phone_number_of_resident#543, housing_status:_is_the_resident_a...__select_as_many_as_apply_#544, job_description#545, which_trade_needs_to_respond_to_repair?#546, what_is_the_priority_for_the_repair?#547, date_of_appointment#548, time_of_appointment#549, have_you_read_the_coronavirus_statement_to_the_resident?_please_advise_the_resident_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing#550, make_a_note_if_the_resident_is_reporting_any_coronavirus_symptoms_in_the_household_and_advise_residents_to_wear_a_face_mask_when_the_operative_is_in_the_property_and_to_maintain_social_distancing#551, is_the_job_a_recharge_or_sus_recharge?#552, is_there_a_cautionary_contact_alert_at_this_address?#553, if_there_is_a_cautionary_contact_alert__what_is_the_nature_of_it?#554, does_the_resident_have_any_vulnerabilities?#555, if_yes__what_vulnerabilities_do_they_have?#556, email_address#557, postcode_of_property#558, uh_phone_number_1#559, uh_phone_number_2#560, block_name#561, ... 9 more fields], false\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convertToNull(dfa):\n",
    "   for i in dfa.columns:\n",
    "       dfa = dfa.withColumn(i , when(col(i) == '', None).otherwise(col(i)))\n",
    "   return dfa\n",
    "\n",
    "def trimAllColumns(dfa):\n",
    "   for c_name in dfa.columns:\n",
    "       dfa = dfa.withColumn(c_name, trim(col(c_name)))\n",
    "   return dfa\n",
    "\n",
    "tmp = df[[\n",
    "    'name_of_resident',\n",
    "    'address_of_repair',\n",
    "    'postcode_of_property',\n",
    "    'import_date',\n",
    "    'import_timestamp',\n",
    "    'import_year'\n",
    "]]\n",
    "tmp = convertToNull(tmp)\n",
    "tmp = trimAllColumns(tmp)\n",
    "\n",
    "tmp = tmp.na.drop(subset=[\"name_of_resident\"])\n",
    "tmp = tmp.sort(col('name_of_resident'), col('import_date'))\n",
    "#tmp = tmp.select(max('import_year'))\n",
    "\n",
    "tmp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('adding new column')\n",
    "df = df.withColumn('address', F.col(source_address_column))\n",
    "\n",
    "logger.info('extract postcode into a new column')\n",
    "df = df.withColumn('postcode', F.regexp_extract(F.col('address'), '([A-Za-z][A-Ha-hJ-Yj-y]?[0-9][A-Za-z0-9]? ?[0-9][A-Za-z]{2}|[Gg][Ii][Rr] ?0[Aa]{2})', 1))\n",
    "\n",
    "logger.info('remove postcode from address')\n",
    "df = df.withColumn('address', F.regexp_replace(F.col('address'), '([A-Za-z][A-Ha-hJ-Yj-y]?[0-9][A-Za-z0-9]? ?[0-9][A-Za-z]{2}|[Gg][Ii][Rr] ?0[Aa]{2})', ''))\n",
    "\n",
    "logger.info('populate empty postcode with postcode from the other PC column')\n",
    "df = df.withColumn(\"postcode\", \\\n",
    "       F.when(F.col(\"postcode\")==\"\" ,None) \\\n",
    "          .otherwise(F.col(\"postcode\")))\n",
    "\n",
    "if source_postcode_column:\n",
    "    df = df.withColumn(\"postcode\", F.coalesce(F.col('postcode'),F.col(source_postcode_column)))\n",
    "\n",
    "logger.info('postcode formatting')\n",
    "df = df.withColumn(\"postcode\", F.upper(F.col(\"postcode\")))\n",
    "df = df.withColumn(\"postcode_nospace\", F.regexp_replace(F.col(\"postcode\"), \" +\", \"\"))\n",
    "df = df.withColumn(\"postcode_length\", F.length(F.col(\"postcode_nospace\"))) \n",
    "df = df.withColumn(\"postcode_start\", F.expr(\"substring(postcode_nospace, 1, postcode_length -3)\"))\n",
    "df = df.withColumn(\"postcode_end\", F.expr(\"substring(postcode_nospace, -3, 3)\"))\n",
    "df = df.withColumn(\"postcode\", F.concat_ws(\" \", \"postcode_start\", \"postcode_end\"))\n",
    "\n",
    "logger.info('address line formatting - remove commas and extra spaces')\n",
    "df = df.withColumn(\"address\", F.upper(F.col(\"address\")))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \",\", \"\"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" +\", \" \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" ?- ?\\z\", \"\"))\n",
    "\n",
    "\n",
    "logger.info('address line formatting - remove LONDON at the end (dont do this for out of London matching)')\n",
    "df = df.withColumn(\"address\", F.trim(F.col(\"address\")))\n",
    "df = df.withColumn(\"address_length\", F.length(F.col(\"address\"))) \n",
    "df = df.withColumn(\"address\", \\\n",
    "       F.when(F.col(\"address\").endswith(\" LONDON\"), F.expr(\"substring(address, 1, address_length -7)\")) \\\n",
    "          .otherwise(F.col(\"address\")))\n",
    "                   \n",
    "logger.info('address line formatting - remove HACKNEY at the end (dont necessarily this for out of borough matching)')\n",
    "df = df.withColumn(\"address\", F.trim(F.col(\"address\")))\n",
    "df = df.withColumn(\"address_length\", F.length(F.col(\"address\"))) \n",
    "df = df.withColumn(\"address\", \\\n",
    "       F.when(F.col(\"address\").endswith(\" HACKNEY\"), F.expr(\"substring(address, 1, address_length -8)\")) \\\n",
    "          .otherwise(F.col(\"address\")))\n",
    "\n",
    "logger.info('address line formatting - dashes between numbers: remove extra spaces')\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), '(\\\\d+) ?- ?(\\\\d+)', '$1-$2'))\n",
    "\n",
    "logger.info('deal with abreviations')\n",
    "\n",
    "logger.info('for \\'street\\': we only replace st if it is at the end of the string, if not there is a risk of confusion with saint')\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" ST.?\\z\", \" STREET\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" RD.? \", \" ROAD \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" RD.?\\z\", \" ROAD\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" AVE \", \" AVENUE \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" AVE\\z\", \" AVENUE\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" HSE \", \" HOUSE \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" HSE\\z\", \" HOUSE\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" CT.? \", \" COURT \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" CT.?\\z\", \" COURT\"))\n",
    "\n",
    "# df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" ST.? \", \" SAINT \"))\n",
    "\n",
    "df = df.withColumnRenamed(\"address\", \"concatenated_string_to_match\")\n",
    "\n",
    "logger.info('create a unique ID')\n",
    "df = df.withColumn(\"prinx\", F.monotonically_increasing_id()).repartition(1)\n",
    "\n",
    "logger.info('write into parquet')\n",
    "cleanedDataframe = DynamicFrame.fromDF(df, glueContext, \"cleanedDataframe\")\n",
    "\n",
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=cleanedDataframe,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": cleaned_addresses_s3_bucket_target, \"partitionKeys\": []},\n",
    "    transformation_ctx=\"parquetData\")\n",
    "\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
