{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('address-cleaning')\n",
    "\n",
    "sys.argv.append('--cleaned_addresses_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-refined-zone/housing/repairs-dlo/cleanedAddresses')\n",
    "\n",
    "sys.argv.append('--source_catalog_database')\n",
    "sys.argv.append('dataplatform-stg-raw-zone-database')\n",
    "\n",
    "sys.argv.append('--source_catalog_table')\n",
    "sys.argv.append('housing_repairs_dlo')\n",
    "\n",
    "sys.argv.append('--source_address_column_header')\n",
    "sys.argv.append('address_of_repair')\n",
    "\n",
    "sys.argv.append('--source_postcode_column_header')\n",
    "sys.argv.append('postcode_of_property')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, HashingTF, MinHashLSH\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, trim, when, max\n",
    "import pyspark.sql.functions as F\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "    \n",
    "def getLatestPartitions(dfa):\n",
    "   dfa = dfa.where(col('import_year') == dfa.select(max('import_year')).first()[0])\n",
    "   dfa = dfa.where(col('import_month') == dfa.select(max('import_month')).first()[0])\n",
    "   dfa = dfa.where(col('import_day') == dfa.select(max('import_day')).first()[0])\n",
    "   return dfa\n",
    "\n",
    "## write into the log file with:\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "source_catalog_table = get_glue_env_var('source_catalog_table', '')\n",
    "source_address_column_header = get_glue_env_var('source_address_column_header', '')\n",
    "source_postcode_column_header = get_glue_env_var('source_postcode_column_header', '')\n",
    "cleaned_addresses_s3_bucket_target = get_glue_env_var('cleaned_addresses_s3_bucket_target', '')\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "\"cannot resolve '`import_year`' given input columns: [];;\\n'Aggregate [max('import_year) AS max(import_year)#2]\\n+- LogicalRDD false\\n\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 21, in getLatestPartitions\n",
      "  File \"/home/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1320, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/home/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/home/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`import_year`' given input columns: [];;\\n'Aggregate [max('import_year) AS max(import_year)#2]\\n+- LogicalRDD false\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Fetch Source Data')\n",
    "source_dataset = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=source_catalog_database,\n",
    "    table_name=source_catalog_table,\n",
    "    transformation_ctx='source_dataset',\n",
    ")\n",
    "\n",
    "df = source_dataset.toDF()\n",
    "source_dataset.printSchema()\n",
    "\n",
    "df = getLatestPartitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[[\n",
    "    'name_of_resident',\n",
    "    'address_of_repair',\n",
    "    'postcode_of_property',\n",
    "    'import_datetime',\n",
    "    'import_date',\n",
    "    'import_timestamp',\n",
    "    'import_year'\n",
    "]]\n",
    "\n",
    "tmp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('adding new column')\n",
    "df = df.withColumn('address', F.col(source_address_column_header))\n",
    "\n",
    "logger.info('extract postcode into a new column')\n",
    "df = df.withColumn('postcode', F.regexp_extract(F.col('address'), '([A-Za-z][A-Ha-hJ-Yj-y]?[0-9][A-Za-z0-9]? ?[0-9][A-Za-z]{2}|[Gg][Ii][Rr] ?0[Aa]{2})', 1))\n",
    "\n",
    "logger.info('remove postcode from address')\n",
    "df = df.withColumn('address', F.regexp_replace(F.col('address'), '([A-Za-z][A-Ha-hJ-Yj-y]?[0-9][A-Za-z0-9]? ?[0-9][A-Za-z]{2}|[Gg][Ii][Rr] ?0[Aa]{2})', ''))\n",
    "\n",
    "logger.info('populate empty postcode with postcode from the other PC column')\n",
    "df = df.withColumn(\"postcode\", \\\n",
    "       F.when(F.col(\"postcode\")==\"\" ,None) \\\n",
    "          .otherwise(F.col(\"postcode\")))\n",
    "\n",
    "if source_postcode_column_header:\n",
    "    df = df.withColumn(\"postcode\", F.coalesce(F.col('postcode'),F.col(source_postcode_column_header)))\n",
    "\n",
    "logger.info('postcode formatting')\n",
    "df = df.withColumn(\"postcode\", F.upper(F.col(\"postcode\")))\n",
    "df = df.withColumn(\"postcode_nospace\", F.regexp_replace(F.col(\"postcode\"), \" +\", \"\"))\n",
    "df = df.withColumn(\"postcode_length\", F.length(F.col(\"postcode_nospace\"))) \n",
    "df = df.withColumn(\"postcode_start\", F.expr(\"substring(postcode_nospace, 1, postcode_length -3)\"))\n",
    "df = df.withColumn(\"postcode_end\", F.expr(\"substring(postcode_nospace, -3, 3)\"))\n",
    "df = df.withColumn(\"postcode\", F.concat_ws(\" \", \"postcode_start\", \"postcode_end\"))\n",
    "\n",
    "logger.info('address line formatting - remove commas and extra spaces')\n",
    "df = df.withColumn(\"address\", F.upper(F.col(\"address\")))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \",\", \"\"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" +\", \" \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" ?- ?\\z\", \"\"))\n",
    "\n",
    "\n",
    "logger.info('address line formatting - remove LONDON at the end (dont do this for out of London matching)')\n",
    "df = df.withColumn(\"address\", F.trim(F.col(\"address\")))\n",
    "df = df.withColumn(\"address_length\", F.length(F.col(\"address\"))) \n",
    "df = df.withColumn(\"address\", \\\n",
    "       F.when(F.col(\"address\").endswith(\" LONDON\"), F.expr(\"substring(address, 1, address_length -7)\")) \\\n",
    "          .otherwise(F.col(\"address\")))\n",
    "                   \n",
    "logger.info('address line formatting - remove HACKNEY at the end (dont necessarily this for out of borough matching)')\n",
    "df = df.withColumn(\"address\", F.trim(F.col(\"address\")))\n",
    "df = df.withColumn(\"address_length\", F.length(F.col(\"address\"))) \n",
    "df = df.withColumn(\"address\", \\\n",
    "       F.when(F.col(\"address\").endswith(\" HACKNEY\"), F.expr(\"substring(address, 1, address_length -8)\")) \\\n",
    "          .otherwise(F.col(\"address\")))\n",
    "\n",
    "logger.info('address line formatting - dashes between numbers: remove extra spaces')\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), '(\\\\d+) ?- ?(\\\\d+)', '$1-$2'))\n",
    "\n",
    "logger.info('deal with abbreviations')\n",
    "\n",
    "logger.info('for \\'street\\': we only replace st if it is at the end of the string, if not there is a risk of confusion with saint')\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" ST.?\\z\", \" STREET\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" RD.? \", \" ROAD \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" RD.?\\z\", \" ROAD\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" AVE \", \" AVENUE \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" AVE\\z\", \" AVENUE\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" HSE \", \" HOUSE \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" HSE\\z\", \" HOUSE\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" CT.? \", \" COURT \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" CT.?\\z\", \" COURT\"))\n",
    "\n",
    "# df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" ST.? \", \" SAINT \"))\n",
    "\n",
    "df = df.withColumnRenamed(\"address\", \"concatenated_string_to_match\")\n",
    "\n",
    "logger.info('create a unique ID')\n",
    "df = df.withColumn(\"prinx\", F.monotonically_increasing_id()).repartition(1)\n",
    "\n",
    "logger.info('write into parquet')\n",
    "cleanedDataframe = DynamicFrame.fromDF(df, glueContext, \"cleanedDataframe\")\n",
    "\n",
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=cleanedDataframe,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": cleaned_addresses_s3_bucket_target, \"partitionKeys\": [\"import_year\", \"import_month\", \"import_day\", \"import_date\"]},\n",
    "    transformation_ctx=\"parquetData\")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
