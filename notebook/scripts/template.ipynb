{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv.append('--source_catalog_table')\n",
    "# TODO: set name of the source table name in the glue catalog \n",
    "sys.argv.append('dataset-name')\n",
    "\n",
    "sys.argv.append('--source_catalog_database')\n",
    "# TODO: set name of the source database name in the glue catalog \n",
    "sys.argv.append('dataplatform-stg-refined-zone-database')\n",
    "\n",
    "sys.argv.append('--s3_bucket_target')\n",
    "# TODO: set S3 location where the transformed data will be saved\n",
    "sys.argv.append('s3://bucket-name/path/to/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql.functions import rank, col, trim, when, max\n",
    "import pyspark.sql.functions as F\n",
    "# from helper.helpers import get_glue_env_var,\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "job = Job(glueContext)\n",
    "\n",
    "source_catalog_table = get_glue_env_var('source_catalog_table', '')\n",
    "source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "target_bucket = get_glue_env_var('s3_bucket_target', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from glue catalog\n",
    "data_source = glueContext.create_dynamic_frame.from_catalog(\n",
    "    name_space=source_catalog_database,\n",
    "    table_name=source_catalog_table\n",
    ")\n",
    "\n",
    "# convert to a data frame\n",
    "df = data_source.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data frame to dynamic frame \n",
    "dyanmic_frame = DynamicFrame.fromDF(df, glueContext, \"target_data_to_write\")\n",
    "\n",
    "# Write the data to S3\n",
    "parquet_data = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=dyanmic_frame,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": target_bucket, \"partitionKeys\": [\"import_year\", \"import_month\", \"import_day\", \"import_date\"]},\n",
    "    transformation_ctx=\"target_data_to_write\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue Spark - Local (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
