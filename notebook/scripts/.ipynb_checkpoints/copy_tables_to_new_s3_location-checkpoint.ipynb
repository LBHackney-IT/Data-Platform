{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv.append('--source_catalog_database')\n",
    "sys.argv.append('delete_me')\n",
    "\n",
    "sys.argv.append('--source_catalog_table')\n",
    "sys.argv.append(['delete_me_repairs_alpha_track', 'delete_me_repairs_avonline', 'delete_me_repairs_dlo', 'delete_me_repairs_stannah', 'delete_me_test_repairs_door_entry', 'delete_me_test_repairs_herts_heritage', 'delete_me_test_repairs_lightning_protection'])\n",
    "\n",
    "sys.argv.append('--s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-raw-zone/housing-repairs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql.functions import rank, col, trim, when, max\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "    \n",
    "def getLatestPartitions(dfa):\n",
    "   dfa = dfa.where(col('import_year') == dfa.select(max('import_year')).first()[0])\n",
    "   dfa = dfa.where(col('import_month') == dfa.select(max('import_month')).first()[0])\n",
    "   dfa = dfa.where(col('import_day') <= '23')\n",
    "   return dfa\n",
    "\n",
    "def replace_hyphens(output_path):\n",
    "    output_path = output_path.replace(\"_\", \"-\")\n",
    "    return output_path\n",
    "\n",
    "def get_output_folder_name(table_name):\n",
    "    if \"test\" in table_name:\n",
    "        output_table_name = table_name[15:]\n",
    "    else:\n",
    "        output_table_name = table_name[10:]\n",
    "    return replace_hyphens(output_table_name)\n",
    "        \n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "job = Job(glueContext)\n",
    "   \n",
    "source_catalog_table = get_glue_env_var('source_catalog_table', '')\n",
    "source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "target_bucket = get_glue_env_var('s3_bucket_target', '')          \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in source_catalog_table:\n",
    "\n",
    "    data_source = glueContext.create_dynamic_frame.from_catalog(\n",
    "        name_space= source_catalog_database,\n",
    "        table_name= table\n",
    "        )\n",
    "\n",
    "    # convert to data frame\n",
    "\n",
    "    df = data_source.toDF()\n",
    "    df = getLatestPartitions(df)\n",
    "\n",
    "    df = df.withColumnRenamed(\"import_date\",\"import_datetime\")\n",
    "    df = df.withColumn(\"import_date\", F.concat('import_year', 'import_month', 'import_day'))\n",
    "\n",
    "    # convert back to dynamic frame\n",
    "\n",
    "    tmp = df[[\n",
    "        'import_day',\n",
    "        'import_datetime'\n",
    "    ]]\n",
    "    tmp.show()\n",
    "    data_with_renamed_headers = DynamicFrame.fromDF(df, glueContext, \"data_with_renamed_headers\")\n",
    "    \n",
    "    new_target_bucket = target_bucket + get_output_folder_name(table)\n",
    "    \n",
    "    \n",
    "    print(new_target_bucket)\n",
    "\n",
    "    parquet_data = glueContext.write_dynamic_frame.from_options(\n",
    "        frame=data_with_renamed_headers,\n",
    "        connection_type=\"s3\",\n",
    "        format=\"parquet\",\n",
    "        connection_options={\"path\": new_target_bucket, \"partitionKeys\": [\"import_year\", \"import_month\", \"import_day\", \"import_date\"]},\n",
    "        transformation_ctx=\"data_with_renamed_headers\")\n",
    "\n",
    "\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
