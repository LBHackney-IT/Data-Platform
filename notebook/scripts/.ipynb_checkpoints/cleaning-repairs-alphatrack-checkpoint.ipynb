{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('address-cleaning')\n",
    "\n",
    "sys.argv.append('--source_catalog_database')\n",
    "sys.argv.append('housing-repairs-raw-zone')\n",
    "\n",
    "sys.argv.append('--source_catalog_table')\n",
    "sys.argv.append('housing_repairs_repairs_alpha_track')\n",
    "\n",
    "sys.argv.append('--cleaned_repairs_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-refined-zone/housing-repairs/repairs-alpha-track/cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, HashingTF, MinHashLSH\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, trim, when, max, trim\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "def getLatestPartitions(dfa):\n",
    "   dfa = dfa.where(col('import_year') == dfa.select(max('import_year')).first()[0])\n",
    "   dfa = dfa.where(col('import_month') == dfa.select(max('import_month')).first()[0])\n",
    "   dfa = dfa.where(col('import_day') == dfa.select(max('import_day')).first()[0])\n",
    "   return dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "source_catalog_table    = get_glue_env_var('source_catalog_table', '')\n",
    "cleaned_repairs_s3_bucket_target = get_glue_env_var('cleaned_repairs_s3_bucket_target', '')\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Fetch Source Data')\n",
    "\n",
    "source_data = glueContext.create_dynamic_frame.from_catalog(\n",
    "    name_space=source_catalog_database,\n",
    "    table_name=source_catalog_table,\n",
    "#     push_down_predicate=\"import_date==max(import_date)\"\n",
    ") \n",
    "\n",
    "df = source_data.toDF()\n",
    "df = getLatestPartitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up column names\n",
    "logger.info('clean up column names')\n",
    "df2 = df.toDF(*[c.lower().replace(' ', '_') for c in df.columns])\n",
    "df2 = df2.toDF(*[c.lower().replace('-', '_') for c in df.columns])\n",
    "df2 = df2.toDF(*[c.lower().replace('__', '_') for c in df.columns])\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('convert timestamp and date columns to datetime / date field types')\n",
    "df2 = df2.withColumn('timestamp', F.to_timestamp(\"timestamp\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "df2 = df2.withColumn('date_-_temp_order_reference_', F.to_date('date_-_temp_order_reference_', \"dd/MM/yyyy\"))\n",
    "\n",
    "# convert contact details to title case\n",
    "df2 = df2.withColumn('contact_information', F.initcap(F.col('contact_information')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split out phone number from contact field\n",
    "df2 = df2.withColumn(\"phone_1\", F.regexp_extract(\"contact_information\", \"(\\d+)\", 0))\n",
    "\n",
    "# keep name only from contact field\n",
    "df2 = df2.withColumn(\"name_full\", F.regexp_extract(\"contact_information\", \"[a-zA-Z]+(?:\\s[a-zA-Z]+)*\", 0))\n",
    "\n",
    "# add new data source column to specify which repairs sheet the repair came from\n",
    "df2 = df2.withColumn('data_source', F.lit('Alphatrack'))\n",
    "\n",
    "# rename column names\n",
    "df2 = df2.withColumnRenamed('email_address', 'email_staff') \\\n",
    "    .withColumnRenamed('date_-_temp_order_reference_', 'temp_order_number_date') \\\n",
    "    .withColumnRenamed('time_-_temp_order_reference_', 'temp_order_number_time') \\\n",
    "    .withColumnRenamed('temporary_order_number_if_required_', 'temp_order_number_full') \\\n",
    "    .withColumnRenamed('call_out_sors', 'sor') \\\n",
    "    .withColumnRenamed('priority_code', 'work_priority_description') \\\n",
    "    .withColumnRenamed('notes_and_information', 'notes')\n",
    "#     .withColumnRenamed('contact_information', '') \\\n",
    "\n",
    "# drop columns not needed \n",
    "df2 = df2.drop('contact_information')\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_repair_priority(code):\n",
    "    if code == 'Immediate':\n",
    "        return 1\n",
    "    elif code == 'Emergency':\n",
    "        return 2\n",
    "    elif code == 'Urgent':\n",
    "        return 3\n",
    "    elif code == 'Normal':\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# # convert to a UDF Function by passing in the function and the return type of function (string in this case)\n",
    "udf_map_repair_priority = F.udf(map_repair_priority, StringType())\n",
    "# apply function\n",
    "df2 = df2.withColumn('work_priority_priority_code', udf_map_repair_priority('work_priority_description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDataframe = DynamicFrame.fromDF(df2, glueContext, \"cleanedDataframe\")\n",
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=cleanedDataframe,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": cleaned_repairs_s3_bucket_target,\"partitionKeys\": [\"import_year\", \"import_month\", \"import_day\", \"import_date\"]},\n",
    "    transformation_ctx=\"parquetData\")\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
