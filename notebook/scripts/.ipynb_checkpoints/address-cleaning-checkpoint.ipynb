{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('address-cleaning')\n",
    "\n",
    "sys.argv.append('--cleaned_addresses_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-refined-zone/housing/repairs-DLO/cleanedAddresses')\n",
    "\n",
    "sys.argv.append('--source_catalog_database')\n",
    "sys.argv.append('dataplatform-stg-raw-zone-database')\n",
    "\n",
    "sys.argv.append('--source_catalog_table')\n",
    "sys.argv.append('housing_repairs_dlo')\n",
    "\n",
    "sys.argv.append('--source_address_column_header')\n",
    "sys.argv.append('address_of_repair')\n",
    "\n",
    "sys.argv.append('--source_postcode_column_header')\n",
    "sys.argv.append('postcode_of_property')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, HashingTF, MinHashLSH\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, trim, when, max\n",
    "import pyspark.sql.functions as F\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "    \n",
    "def getLatestPartitions(dfa):\n",
    "   dfa = dfa.where(col('import_year') == dfa.select(max('import_year')).first()[0])\n",
    "   dfa = dfa.where(col('import_month') == dfa.select(max('import_month')).first()[0])\n",
    "   dfa = dfa.where(col('import_day') == dfa.select(max('import_day')).first()[0])\n",
    "   return dfa\n",
    "\n",
    "## write into the log file with:\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "source_catalog_table = get_glue_env_var('source_catalog_table', '')\n",
    "source_address_column_header = get_glue_env_var('source_address_column_header', '')\n",
    "source_postcode_column_header = get_glue_env_var('source_postcode_column_header', '')\n",
    "cleaned_addresses_s3_bucket_target = get_glue_env_var('cleaned_addresses_s3_bucket_target', '')\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Fetch Source Data')\n",
    "source_dataset = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=source_catalog_database,\n",
    "    table_name=source_catalog_table,\n",
    "    transformation_ctx='source_dataset',\n",
    ")\n",
    "\n",
    "df = source_dataset.toDF()\n",
    "source_dataset.printSchema()\n",
    "\n",
    "df = getLatestPartitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[[\n",
    "    'name_of_resident',\n",
    "    'address_of_repair',\n",
    "    'postcode_of_property',\n",
    "    'import_datetime',\n",
    "    'import_date',\n",
    "    'import_timestamp',\n",
    "    'import_year'\n",
    "]]\n",
    "\n",
    "tmp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('adding new column')\n",
    "df = df.withColumn('address', F.col(source_address_column_header))\n",
    "\n",
    "logger.info('extract postcode into a new column')\n",
    "df = df.withColumn('postcode', F.regexp_extract(F.col('address'), '([A-Za-z][A-Ha-hJ-Yj-y]?[0-9][A-Za-z0-9]? ?[0-9][A-Za-z]{2}|[Gg][Ii][Rr] ?0[Aa]{2})', 1))\n",
    "\n",
    "logger.info('remove postcode from address')\n",
    "df = df.withColumn('address', F.regexp_replace(F.col('address'), '([A-Za-z][A-Ha-hJ-Yj-y]?[0-9][A-Za-z0-9]? ?[0-9][A-Za-z]{2}|[Gg][Ii][Rr] ?0[Aa]{2})', ''))\n",
    "\n",
    "logger.info('populate empty postcode with postcode from the other PC column')\n",
    "df = df.withColumn(\"postcode\", \\\n",
    "       F.when(F.col(\"postcode\")==\"\" ,None) \\\n",
    "          .otherwise(F.col(\"postcode\")))\n",
    "\n",
    "if source_postcode_column_header:\n",
    "    df = df.withColumn(\"postcode\", F.coalesce(F.col('postcode'),F.col(source_postcode_column_header)))\n",
    "\n",
    "logger.info('postcode formatting')\n",
    "df = df.withColumn(\"postcode\", F.upper(F.col(\"postcode\")))\n",
    "df = df.withColumn(\"postcode_nospace\", F.regexp_replace(F.col(\"postcode\"), \" +\", \"\"))\n",
    "df = df.withColumn(\"postcode_length\", F.length(F.col(\"postcode_nospace\"))) \n",
    "df = df.withColumn(\"postcode_start\", F.expr(\"substring(postcode_nospace, 1, postcode_length -3)\"))\n",
    "df = df.withColumn(\"postcode_end\", F.expr(\"substring(postcode_nospace, -3, 3)\"))\n",
    "df = df.withColumn(\"postcode\", F.concat_ws(\" \", \"postcode_start\", \"postcode_end\"))\n",
    "\n",
    "logger.info('address line formatting - remove commas and extra spaces')\n",
    "df = df.withColumn(\"address\", F.upper(F.col(\"address\")))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \",\", \"\"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" +\", \" \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" ?- ?\\z\", \"\"))\n",
    "\n",
    "\n",
    "logger.info('address line formatting - remove LONDON at the end (dont do this for out of London matching)')\n",
    "df = df.withColumn(\"address\", F.trim(F.col(\"address\")))\n",
    "df = df.withColumn(\"address_length\", F.length(F.col(\"address\"))) \n",
    "df = df.withColumn(\"address\", \\\n",
    "       F.when(F.col(\"address\").endswith(\" LONDON\"), F.expr(\"substring(address, 1, address_length -7)\")) \\\n",
    "          .otherwise(F.col(\"address\")))\n",
    "                   \n",
    "logger.info('address line formatting - remove HACKNEY at the end (dont necessarily this for out of borough matching)')\n",
    "df = df.withColumn(\"address\", F.trim(F.col(\"address\")))\n",
    "df = df.withColumn(\"address_length\", F.length(F.col(\"address\"))) \n",
    "df = df.withColumn(\"address\", \\\n",
    "       F.when(F.col(\"address\").endswith(\" HACKNEY\"), F.expr(\"substring(address, 1, address_length -8)\")) \\\n",
    "          .otherwise(F.col(\"address\")))\n",
    "\n",
    "logger.info('address line formatting - dashes between numbers: remove extra spaces')\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), '(\\\\d+) ?- ?(\\\\d+)', '$1-$2'))\n",
    "\n",
    "logger.info('deal with abbreviations')\n",
    "\n",
    "logger.info('for \\'street\\': we only replace st if it is at the end of the string, if not there is a risk of confusion with saint')\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" ST.?\\z\", \" STREET\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" RD.? \", \" ROAD \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" RD.?\\z\", \" ROAD\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" AVE \", \" AVENUE \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" AVE\\z\", \" AVENUE\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" HSE \", \" HOUSE \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" HSE\\z\", \" HOUSE\"))\n",
    "\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" CT.? \", \" COURT \"))\n",
    "df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" CT.?\\z\", \" COURT\"))\n",
    "\n",
    "# df = df.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \" ST.? \", \" SAINT \"))\n",
    "\n",
    "df = df.withColumnRenamed(\"address\", \"concatenated_string_to_match\")\n",
    "\n",
    "logger.info('create a unique ID')\n",
    "df = df.withColumn(\"prinx\", F.monotonically_increasing_id()).repartition(1)\n",
    "\n",
    "logger.info('write into parquet')\n",
    "cleanedDataframe = DynamicFrame.fromDF(df, glueContext, \"cleanedDataframe\")\n",
    "\n",
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=cleanedDataframe,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": cleaned_addresses_s3_bucket_target, \"partitionKeys\": [\"import_year\", \"import_month\", \"import_day\", \"import_date\"]},\n",
    "    transformation_ctx=\"parquetData\")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
