{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('address-cleaning')\n",
    "\n",
    "sys.argv.append('--source_catalog_database')\n",
    "sys.argv.append('housing-repairs-raw-zone')\n",
    "\n",
    "sys.argv.append('--source_catalog_table')\n",
    "sys.argv.append('housing_repairs_lightning_protection')\n",
    "\n",
    "sys.argv.append('--cleaned_repairs_s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-refined-zone/housing-repairs/repairs-electrical-mechanical-fire/housing-lightning_protection_/cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, HashingTF, MinHashLSH\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, trim, when, max, trim\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "def getLatestPartitions(dfa):\n",
    "   dfa = dfa.where(col('import_year') == dfa.select(max('import_year')).first()[0])\n",
    "   dfa = dfa.where(col('import_month') == dfa.select(max('import_month')).first()[0])\n",
    "   dfa = dfa.where(col('import_day') == dfa.select(max('import_day')).first()[0])\n",
    "   return dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "source_catalog_table    = get_glue_env_var('source_catalog_table', '')\n",
    "cleaned_repairs_s3_bucket_target = get_glue_env_var('cleaned_repairs_s3_bucket_target', '')\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "logger = glueContext.get_logger()\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info('Fetch Source Data')\n",
    "source_data = glueContext.create_dynamic_frame.from_catalog(\n",
    "    name_space=source_catalog_database,\n",
    "    table_name=source_catalog_table,\n",
    ") \n",
    "\n",
    "df = source_data.toDF()\n",
    "df = getLatestPartitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up column names\n",
    "import re\n",
    "logger.info('clean up column names')\n",
    "def clean_column_names(df):\n",
    "    # remove full stops from column names\n",
    "    df = df.select([F.col(\"`{0}`\".format(c)).alias(\n",
    "        c.replace('.', '')) for c in df.columns])\n",
    "    # remove trialing underscores\n",
    "    df = df.select([F.col(col).alias(re.sub(\"_$\", \"\", col))\n",
    "                   for col in df.columns])\n",
    "    # lowercase and remove double underscores\n",
    "    df2 = df.select([F.col(col).alias(\n",
    "        re.sub(\"[^0-9a-zA-Z$]+\", \"_\", col.lower())) for col in df.columns])\n",
    "    return df2\n",
    "\n",
    "df2 = clean_column_names(df)\n",
    "df2.show(vertical=True, n=200)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn('datetime_raised', F.to_timestamp('date', 'dd.MM.yy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(vertical=True, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new data source column to specify which repairs sheet the repair came from\n",
    "df2 = df2.withColumn('data_source', F.lit('Lighting Protection'))\n",
    "\n",
    "# # rename column names\n",
    "df2 = df2.withColumnRenamed('requested_by', 'operative') \\\n",
    "    .withColumnRenamed('address', 'property_address') \\\n",
    "    .withColumnRenamed('description', 'description_of_work') \\\n",
    "    .withColumnRenamed('priority_code', 'work_priority_description') \\\n",
    "    .withColumnRenamed('temp_order_number', 'temp_order_number_full') \\\n",
    "    .withColumnRenamed('cost', 'order_value')\\\n",
    "    .withColumnRenamed('subjective', 'budget_code')\\\n",
    "    .withColumnRenamed('contractor_s_own_ref_no', 'contractor_ref')\n",
    "\n",
    "df2 = df2.drop('date')\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_repair_priority(code):\n",
    "    if code == 'Immediate':\n",
    "        return 1\n",
    "    elif code == 'Emergency':\n",
    "        return 2\n",
    "    elif code == 'Urgent':\n",
    "        return 3\n",
    "    elif code == 'Normal':\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# # convert to a UDF Function by passing in the function and the return type of function (string in this case)\n",
    "udf_map_repair_priority = F.udf(map_repair_priority, StringType())\n",
    "# apply function\n",
    "df2 = df2.withColumn('work_priority_priority_code', udf_map_repair_priority('work_priority_description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDataframe = DynamicFrame.fromDF(df2, glueContext, \"cleanedDataframe\")\n",
    "parquetData = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=cleanedDataframe,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": cleaned_repairs_s3_bucket_target,\"partitionKeys\": [\"import_year\", \"import_month\", \"import_day\", \"import_date\"]},\n",
    "    transformation_ctx=\"parquetData\")\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
