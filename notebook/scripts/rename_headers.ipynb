{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv.append('--source_catalog_table')\n",
    "sys.argv.append('housing_repairs_stannah')\n",
    "\n",
    "sys.argv.append('--source_catalog_database')\n",
    "sys.argv.append('dataplatform-stg-landing-zone-database')\n",
    "\n",
    "sys.argv.append('--s3_bucket_target')\n",
    "sys.argv.append('s3://dataplatform-stg-raw-zone/housing/repairs-stannah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql.functions import rank, col, trim, when, max\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def get_glue_env_var(key, default=\"none\"):\n",
    "    if f'--{key}' in sys.argv:\n",
    "        return getResolvedOptions(sys.argv, [key])[key]\n",
    "    else:\n",
    "        return default\n",
    "    \n",
    "def getLatestPartitions(dfa):\n",
    "   dfa = dfa.where(col('import_year') == dfa.select(max('import_year')).first()[0])\n",
    "   dfa = dfa.where(col('import_month') == dfa.select(max('import_month')).first()[0])\n",
    "   dfa = dfa.where(col('import_day') == dfa.select(max('import_day')).first()[0])\n",
    "   return dfa\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "job = Job(glueContext)\n",
    "\n",
    "source_catalog_table = get_glue_env_var('source_catalog_table', '')\n",
    "source_catalog_database = get_glue_env_var('source_catalog_database', '')\n",
    "target_bucket = get_glue_env_var('s3_bucket_target', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = glueContext.create_dynamic_frame.from_catalog(\n",
    "    name_space= source_catalog_database,\n",
    "    table_name= source_catalog_table\n",
    "    )\n",
    "\n",
    "# convert to data frame\n",
    "\n",
    "df = data_source.toDF()\n",
    "df = getLatestPartitions(df)\n",
    "\n",
    "df = df.withColumnRenamed(\"import_date\",\"import_datetime\")\n",
    "df = df.withColumn(\"import_date\", F.concat('import_year', 'import_month', 'import_day'))\n",
    "   \n",
    "# convert back to dynamic frame\n",
    "\n",
    "tmp = df[[\n",
    "    'import_day',\n",
    "    'import_datetime'\n",
    "]]\n",
    "tmp.show()\n",
    "data_with_renamed_headers = DynamicFrame.fromDF(df, glueContext, \"data_with_renamed_headers\")\n",
    "\n",
    "parquet_data = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=data_with_renamed_headers,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"parquet\",\n",
    "    connection_options={\"path\": target_bucket, \"partitionKeys\": [\"import_year\", \"import_month\", \"import_day\"]},\n",
    "    transformation_ctx=\"data_with_renamed_headers\")\n",
    "\n",
    "\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
